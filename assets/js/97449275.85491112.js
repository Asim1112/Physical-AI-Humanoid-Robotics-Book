"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[1526],{8453(n,e,i){i.d(e,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}},9227(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var t=i(4848),o=i(8453);const s={},a="Multimodal Integration for Humanoid Robots",r={id:"module-4-vla/multimodal-integration",title:"Multimodal Integration for Humanoid Robots",description:"Overview",source:"@site/docs/module-4-vla/multimodal-integration.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/multimodal-integration",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/multimodal-integration",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-4-vla/multimodal-integration.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Vision Grounding for Language Commands",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vision-grounding"},next:{title:"VLA Deployment for Humanoid Robots",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vla-deployment"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"Multimodal Fusion Architecture",id:"multimodal-fusion-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:2},{value:"Transformer-Based Multimodal Fusion",id:"transformer-based-multimodal-fusion",level:3},{value:"Temporal Alignment",id:"temporal-alignment",level:2},{value:"Synchronization of Asynchronous Modalities",id:"synchronization-of-asynchronous-modalities",level:3},{value:"Uncertainty-Aware Fusion",id:"uncertainty-aware-fusion",level:2},{value:"Multimodal Fusion with Uncertainty Quantification",id:"multimodal-fusion-with-uncertainty-quantification",level:3},{value:"Hierarchical Multimodal Processing",id:"hierarchical-multimodal-processing",level:2},{value:"Multi-Level Fusion Architecture",id:"multi-level-fusion-architecture",level:3},{value:"Attention Mechanisms for Multimodal Fusion",id:"attention-mechanisms-for-multimodal-fusion",level:2},{value:"Gated Fusion with Attention",id:"gated-fusion-with-attention",level:3},{value:"Complete Multimodal VLA System",id:"complete-multimodal-vla-system",level:2},{value:"End-to-End Integration",id:"end-to-end-integration",level:3},{value:"Training Strategies",id:"training-strategies",level:2},{value:"Multi-Task Learning for Multimodal Systems",id:"multi-task-learning-for-multimodal-systems",level:3},{value:"Best Practices",id:"best-practices",level:2}];function u(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"multimodal-integration-for-humanoid-robots",children:"Multimodal Integration for Humanoid Robots"}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Multimodal integration combines vision, language, proprioception, and other sensory modalities to create a comprehensive understanding of the robot's state and environment. This chapter explores advanced techniques for fusing multiple information sources, enabling humanoid robots to make informed decisions based on diverse inputs."}),"\n",(0,t.jsx)(e.h2,{id:"multimodal-fusion-architecture",children:"Multimodal Fusion Architecture"}),"\n",(0,t.jsx)(e.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  MULTIMODAL INTEGRATION SYSTEM                  \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502  VISION  \u2502  \u2502 LANGUAGE \u2502  \u2502  PROPRIO \u2502  \u2502  HAPTIC  \u2502      \u2502\n\u2502  \u2502          \u2502  \u2502          \u2502  \u2502  CEPTION \u2502  \u2502          \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502       \u2502             \u2502              \u2502             \u2502             \u2502\n\u2502       \u25bc             \u25bc              \u25bc             \u25bc             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502  VISUAL  \u2502  \u2502 LANGUAGE \u2502  \u2502  JOINT   \u2502  \u2502  FORCE   \u2502      \u2502\n\u2502  \u2502 ENCODER  \u2502  \u2502 ENCODER  \u2502  \u2502 ENCODER  \u2502  \u2502 ENCODER  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502       \u2502             \u2502              \u2502             \u2502             \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                     \u25bc              \u25bc                           \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502            \u2502   MULTIMODAL FUSION     \u2502                         \u2502\n\u2502            \u2502                         \u2502                         \u2502\n\u2502            \u2502  \u2022 Cross-Attention      \u2502                         \u2502\n\u2502            \u2502  \u2022 Feature Alignment    \u2502                         \u2502\n\u2502            \u2502  \u2022 Temporal Sync        \u2502                         \u2502\n\u2502            \u2502  \u2022 Uncertainty Handling \u2502                         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                         \u25bc                                       \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502            \u2502   UNIFIED               \u2502                         \u2502\n\u2502            \u2502   REPRESENTATION        \u2502                         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                         \u25bc                                       \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502            \u2502   ACTION GENERATION     \u2502                         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h2,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,t.jsx)(e.h3,{id:"transformer-based-multimodal-fusion",children:"Transformer-Based Multimodal Fusion"}),"\n",(0,t.jsx)(e.p,{children:"Implement cross-attention between different modalities:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom typing import Dict, List, Optional\n\nclass MultimodalTransformer(nn.Module):\n    \"\"\"\n    Transformer-based multimodal fusion for VLA systems.\n    Supports vision, language, and proprioceptive inputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int = 768,\n        num_heads: int = 12,\n        num_layers: int = 6,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n\n        # Modality-specific projections\n        self.vision_projection = nn.Linear(2048, hidden_dim)  # From ResNet\n        self.language_projection = nn.Linear(768, hidden_dim)  # From BERT\n        self.proprio_projection = nn.Linear(64, hidden_dim)   # Joint states + IMU\n        self.haptic_projection = nn.Linear(32, hidden_dim)    # Force/torque sensors\n\n        # Positional encodings for each modality\n        self.vision_pos_encoding = nn.Parameter(torch.randn(1, 196, hidden_dim))  # 14x14 patches\n        self.language_pos_encoding = nn.Parameter(torch.randn(1, 512, hidden_dim))\n        self.proprio_pos_encoding = nn.Parameter(torch.randn(1, 100, hidden_dim))\n\n        # Modality embeddings (learnable tokens to identify modality type)\n        self.modality_embeddings = nn.ParameterDict({\n            'vision': nn.Parameter(torch.randn(1, 1, hidden_dim)),\n            'language': nn.Parameter(torch.randn(1, 1, hidden_dim)),\n            'proprio': nn.Parameter(torch.randn(1, 1, hidden_dim)),\n            'haptic': nn.Parameter(torch.randn(1, 1, hidden_dim))\n        })\n\n        # Cross-modal transformer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Output projection\n        self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n\n    def forward(\n        self,\n        vision_features: Optional[torch.Tensor] = None,\n        language_features: Optional[torch.Tensor] = None,\n        proprio_features: Optional[torch.Tensor] = None,\n        haptic_features: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Fuse multiple modalities.\n\n        Args:\n            vision_features: [batch, seq_len, vision_dim]\n            language_features: [batch, seq_len, language_dim]\n            proprio_features: [batch, seq_len, proprio_dim]\n            haptic_features: [batch, seq_len, haptic_dim]\n            attention_mask: Optional attention mask\n\n        Returns:\n            Dictionary with fused features and modality-specific outputs\n        \"\"\"\n        batch_size = (\n            vision_features.size(0) if vision_features is not None\n            else language_features.size(0) if language_features is not None\n            else proprio_features.size(0)\n        )\n\n        # Collect all modality sequences\n        modality_sequences = []\n        modality_masks = []\n\n        # Process vision\n        if vision_features is not None:\n            vision_proj = self.vision_projection(vision_features)\n            seq_len = vision_proj.size(1)\n            vision_proj = vision_proj + self.vision_pos_encoding[:, :seq_len, :]\n            vision_proj = vision_proj + self.modality_embeddings['vision']\n            modality_sequences.append(vision_proj)\n            modality_masks.append(torch.ones(batch_size, seq_len, device=vision_proj.device))\n\n        # Process language\n        if language_features is not None:\n            language_proj = self.language_projection(language_features)\n            seq_len = language_proj.size(1)\n            language_proj = language_proj + self.language_pos_encoding[:, :seq_len, :]\n            language_proj = language_proj + self.modality_embeddings['language']\n            modality_sequences.append(language_proj)\n            modality_masks.append(torch.ones(batch_size, seq_len, device=language_proj.device))\n\n        # Process proprioception\n        if proprio_features is not None:\n            proprio_proj = self.proprio_projection(proprio_features)\n            seq_len = proprio_proj.size(1)\n            proprio_proj = proprio_proj + self.proprio_pos_encoding[:, :seq_len, :]\n            proprio_proj = proprio_proj + self.modality_embeddings['proprio']\n            modality_sequences.append(proprio_proj)\n            modality_masks.append(torch.ones(batch_size, seq_len, device=proprio_proj.device))\n\n        # Process haptic\n        if haptic_features is not None:\n            haptic_proj = self.haptic_projection(haptic_features)\n            haptic_proj = haptic_proj + self.modality_embeddings['haptic']\n            modality_sequences.append(haptic_proj)\n            modality_masks.append(torch.ones(batch_size, haptic_proj.size(1), device=haptic_proj.device))\n\n        # Concatenate all modalities\n        fused_sequence = torch.cat(modality_sequences, dim=1)  # [batch, total_seq_len, hidden_dim]\n\n        # Create attention mask if needed\n        if attention_mask is None:\n            attention_mask = torch.cat(modality_masks, dim=1)\n\n        # Apply transformer\n        fused_output = self.transformer(\n            fused_sequence,\n            src_key_padding_mask=(attention_mask == 0)\n        )\n\n        # Project output\n        fused_output = self.output_projection(fused_output)\n\n        # Split back into modality-specific outputs\n        outputs = {\n            'fused': fused_output,\n            'pooled': fused_output.mean(dim=1)  # Global pooling\n        }\n\n        # Extract modality-specific representations\n        start_idx = 0\n        if vision_features is not None:\n            end_idx = start_idx + vision_features.size(1)\n            outputs['vision'] = fused_output[:, start_idx:end_idx, :]\n            start_idx = end_idx\n\n        if language_features is not None:\n            end_idx = start_idx + language_features.size(1)\n            outputs['language'] = fused_output[:, start_idx:end_idx, :]\n            start_idx = end_idx\n\n        if proprio_features is not None:\n            end_idx = start_idx + proprio_features.size(1)\n            outputs['proprio'] = fused_output[:, start_idx:end_idx, :]\n            start_idx = end_idx\n\n        if haptic_features is not None:\n            outputs['haptic'] = fused_output[:, start_idx:, :]\n\n        return outputs\n"})}),"\n",(0,t.jsx)(e.h2,{id:"temporal-alignment",children:"Temporal Alignment"}),"\n",(0,t.jsx)(e.h3,{id:"synchronization-of-asynchronous-modalities",children:"Synchronization of Asynchronous Modalities"}),"\n",(0,t.jsx)(e.p,{children:"Handle different sensor update rates:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from collections import deque\nimport time\n\nclass TemporalAligner:\n    \"\"\"\n    Align multimodal inputs with different sampling rates.\n    \"\"\"\n\n    def __init__(self, buffer_size: int = 100):\n        self.buffers = {\n            'vision': deque(maxlen=buffer_size),\n            'language': deque(maxlen=buffer_size),\n            'proprio': deque(maxlen=buffer_size),\n            'haptic': deque(maxlen=buffer_size)\n        }\n\n        # Expected update rates (Hz)\n        self.update_rates = {\n            'vision': 30.0,\n            'language': 10.0,  # Commands come sporadically\n            'proprio': 100.0,\n            'haptic': 100.0\n        }\n\n    def add_sample(self, modality: str, data: torch.Tensor, timestamp: float):\n        \"\"\"Add a sample to the appropriate buffer.\"\"\"\n        self.buffers[modality].append({\n            'data': data,\n            'timestamp': timestamp\n        })\n\n    def get_synchronized_batch(\n        self,\n        target_timestamp: float,\n        time_window: float = 0.1\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Get synchronized batch of multimodal data.\n\n        Args:\n            target_timestamp: Target synchronization time\n            time_window: Tolerance window for synchronization (seconds)\n\n        Returns:\n            Dictionary of synchronized modality data\n        \"\"\"\n        synchronized = {}\n\n        for modality, buffer in self.buffers.items():\n            if not buffer:\n                synchronized[modality] = None\n                continue\n\n            # Find closest sample within time window\n            closest_sample = None\n            min_time_diff = float('inf')\n\n            for sample in buffer:\n                time_diff = abs(sample['timestamp'] - target_timestamp)\n                if time_diff < min_time_diff and time_diff <= time_window:\n                    min_time_diff = time_diff\n                    closest_sample = sample\n\n            if closest_sample is not None:\n                synchronized[modality] = closest_sample['data']\n            else:\n                # Interpolate if no exact match\n                synchronized[modality] = self._interpolate_sample(\n                    modality, target_timestamp\n                )\n\n        return synchronized\n\n    def _interpolate_sample(\n        self,\n        modality: str,\n        target_timestamp: float\n    ) -> Optional[torch.Tensor]:\n        \"\"\"Interpolate sample at target timestamp.\"\"\"\n        buffer = self.buffers[modality]\n\n        if len(buffer) < 2:\n            return None\n\n        # Find samples before and after target timestamp\n        before = None\n        after = None\n\n        for sample in buffer:\n            if sample['timestamp'] <= target_timestamp:\n                if before is None or sample['timestamp'] > before['timestamp']:\n                    before = sample\n            if sample['timestamp'] >= target_timestamp:\n                if after is None or sample['timestamp'] < after['timestamp']:\n                    after = sample\n\n        if before is None or after is None:\n            return None\n\n        # Linear interpolation\n        t_diff = after['timestamp'] - before['timestamp']\n        if t_diff == 0:\n            return before['data']\n\n        alpha = (target_timestamp - before['timestamp']) / t_diff\n        interpolated = (1 - alpha) * before['data'] + alpha * after['data']\n\n        return interpolated\n"})}),"\n",(0,t.jsx)(e.h2,{id:"uncertainty-aware-fusion",children:"Uncertainty-Aware Fusion"}),"\n",(0,t.jsx)(e.h3,{id:"multimodal-fusion-with-uncertainty-quantification",children:"Multimodal Fusion with Uncertainty Quantification"}),"\n",(0,t.jsx)(e.p,{children:"Incorporate uncertainty estimates in fusion:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class UncertaintyAwareFusion(nn.Module):\n    \"\"\"\n    Multimodal fusion that accounts for modality-specific uncertainties.\n    \"\"\"\n\n    def __init__(self, hidden_dim: int = 768):\n        super().__init__()\n\n        # Uncertainty estimation networks for each modality\n        self.vision_uncertainty = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Softplus()  # Ensure positive uncertainty\n        )\n\n        self.language_uncertainty = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Softplus()\n        )\n\n        self.proprio_uncertainty = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Softplus()\n        )\n\n        # Fusion network\n        self.fusion_network = nn.Sequential(\n            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n            nn.LayerNorm(hidden_dim * 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim * 2, hidden_dim)\n        )\n\n    def forward(\n        self,\n        vision_features: torch.Tensor,\n        language_features: torch.Tensor,\n        proprio_features: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Fuse features with uncertainty weighting.\n\n        Args:\n            vision_features: [batch, hidden_dim]\n            language_features: [batch, hidden_dim]\n            proprio_features: [batch, hidden_dim]\n\n        Returns:\n            Dictionary with fused features and uncertainties\n        \"\"\"\n        # Estimate uncertainties\n        vision_unc = self.vision_uncertainty(vision_features)  # [batch, 1]\n        language_unc = self.language_uncertainty(language_features)\n        proprio_unc = self.proprio_uncertainty(proprio_features)\n\n        # Compute weights (inverse uncertainty)\n        vision_weight = 1.0 / (vision_unc + 1e-6)\n        language_weight = 1.0 / (language_unc + 1e-6)\n        proprio_weight = 1.0 / (proprio_unc + 1e-6)\n\n        # Normalize weights\n        total_weight = vision_weight + language_weight + proprio_weight\n        vision_weight = vision_weight / total_weight\n        language_weight = language_weight / total_weight\n        proprio_weight = proprio_weight / total_weight\n\n        # Weighted fusion\n        weighted_vision = vision_features * vision_weight\n        weighted_language = language_features * language_weight\n        weighted_proprio = proprio_features * proprio_weight\n\n        # Concatenate and fuse\n        concatenated = torch.cat([\n            weighted_vision,\n            weighted_language,\n            weighted_proprio\n        ], dim=-1)\n\n        fused_features = self.fusion_network(concatenated)\n\n        return {\n            'fused': fused_features,\n            'vision_uncertainty': vision_unc,\n            'language_uncertainty': language_unc,\n            'proprio_uncertainty': proprio_unc,\n            'vision_weight': vision_weight,\n            'language_weight': language_weight,\n            'proprio_weight': proprio_weight\n        }\n"})}),"\n",(0,t.jsx)(e.h2,{id:"hierarchical-multimodal-processing",children:"Hierarchical Multimodal Processing"}),"\n",(0,t.jsx)(e.h3,{id:"multi-level-fusion-architecture",children:"Multi-Level Fusion Architecture"}),"\n",(0,t.jsx)(e.p,{children:"Fuse at multiple levels of abstraction:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class HierarchicalMultimodalFusion(nn.Module):\n    \"\"\"\n    Hierarchical fusion at multiple levels of abstraction.\n    \"\"\"\n\n    def __init__(self, hidden_dim: int = 768):\n        super().__init__()\n\n        # Low-level fusion (feature-level)\n        self.low_level_fusion = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Mid-level fusion (semantic-level)\n        self.mid_level_fusion = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # High-level fusion (decision-level)\n        self.high_level_fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n            nn.LayerNorm(hidden_dim * 2),\n            nn.GELU(),\n            nn.Linear(hidden_dim * 2, hidden_dim)\n        )\n\n        # Level-specific processing\n        self.vision_processor = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(hidden_dim, 8, hidden_dim * 4, batch_first=True),\n            num_layers=2\n        )\n\n        self.language_processor = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(hidden_dim, 8, hidden_dim * 4, batch_first=True),\n            num_layers=2\n        )\n\n        self.proprio_processor = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(hidden_dim, 8, hidden_dim * 4, batch_first=True),\n            num_layers=2\n        )\n\n    def forward(\n        self,\n        vision_features: torch.Tensor,\n        language_features: torch.Tensor,\n        proprio_features: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Hierarchical fusion across multiple levels.\n\n        Args:\n            vision_features: [batch, seq_len, hidden_dim]\n            language_features: [batch, seq_len, hidden_dim]\n            proprio_features: [batch, seq_len, hidden_dim]\n\n        Returns:\n            Multi-level fused representations\n        \"\"\"\n        # Low-level fusion (early fusion at feature level)\n        # Concatenate sequences\n        low_level_seq = torch.cat([\n            vision_features,\n            language_features,\n            proprio_features\n        ], dim=1)\n\n        # Self-attention across all modalities\n        low_level_fused, _ = self.low_level_fusion(\n            low_level_seq, low_level_seq, low_level_seq\n        )\n\n        # Mid-level fusion (process each modality then fuse)\n        vision_processed = self.vision_processor(vision_features)\n        language_processed = self.language_processor(language_features)\n        proprio_processed = self.proprio_processor(proprio_features)\n\n        # Pool each modality\n        vision_pooled = vision_processed.mean(dim=1)\n        language_pooled = language_processed.mean(dim=1)\n        proprio_pooled = proprio_processed.mean(dim=1)\n\n        # Cross-modal attention for mid-level\n        mid_level_seq = torch.stack([\n            vision_pooled,\n            language_pooled,\n            proprio_pooled\n        ], dim=1)  # [batch, 3, hidden_dim]\n\n        mid_level_fused, _ = self.mid_level_fusion(\n            mid_level_seq, mid_level_seq, mid_level_seq\n        )\n\n        # High-level fusion (late fusion at decision level)\n        high_level_input = torch.cat([\n            vision_pooled,\n            language_pooled,\n            proprio_pooled\n        ], dim=-1)\n\n        high_level_fused = self.high_level_fusion(high_level_input)\n\n        return {\n            'low_level': low_level_fused.mean(dim=1),  # Pool over sequence\n            'mid_level': mid_level_fused.mean(dim=1),\n            'high_level': high_level_fused,\n            'vision': vision_pooled,\n            'language': language_pooled,\n            'proprio': proprio_pooled\n        }\n"})}),"\n",(0,t.jsx)(e.h2,{id:"attention-mechanisms-for-multimodal-fusion",children:"Attention Mechanisms for Multimodal Fusion"}),"\n",(0,t.jsx)(e.h3,{id:"gated-fusion-with-attention",children:"Gated Fusion with Attention"}),"\n",(0,t.jsx)(e.p,{children:"Use attention to dynamically weight modalities:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class GatedMultimodalFusion(nn.Module):\n    """\n    Gated fusion mechanism that learns to weight modalities.\n    """\n\n    def __init__(self, hidden_dim: int = 768, num_modalities: int = 3):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_modalities = num_modalities\n\n        # Gating network\n        self.gate_network = nn.Sequential(\n            nn.Linear(hidden_dim * num_modalities, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, num_modalities),\n            nn.Softmax(dim=-1)\n        )\n\n        # Modality-specific transformations\n        self.modality_transforms = nn.ModuleList([\n            nn.Linear(hidden_dim, hidden_dim)\n            for _ in range(num_modalities)\n        ])\n\n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU()\n        )\n\n    def forward(self, modality_features: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n        """\n        Fuse modalities with learned gating.\n\n        Args:\n            modality_features: List of [batch, hidden_dim] tensors\n\n        Returns:\n            Gated fusion result\n        """\n        batch_size = modality_features[0].size(0)\n\n        # Stack modalities\n        stacked_features = torch.stack(modality_features, dim=1)  # [batch, num_mod, hidden_dim]\n\n        # Compute gates\n        concatenated = torch.cat(modality_features, dim=-1)  # [batch, hidden_dim * num_mod]\n        gates = self.gate_network(concatenated)  # [batch, num_modalities]\n\n        # Transform each modality\n        transformed = []\n        for i, (feat, transform) in enumerate(zip(modality_features, self.modality_transforms)):\n            transformed.append(transform(feat))\n\n        transformed = torch.stack(transformed, dim=1)  # [batch, num_mod, hidden_dim]\n\n        # Apply gates\n        gates_expanded = gates.unsqueeze(-1)  # [batch, num_mod, 1]\n        gated_features = transformed * gates_expanded  # [batch, num_mod, hidden_dim]\n\n        # Sum across modalities\n        fused = gated_features.sum(dim=1)  # [batch, hidden_dim]\n\n        # Output projection\n        output = self.output_projection(fused)\n\n        return {\n            \'fused\': output,\n            \'gates\': gates,\n            \'transformed\': transformed\n        }\n'})}),"\n",(0,t.jsx)(e.h2,{id:"complete-multimodal-vla-system",children:"Complete Multimodal VLA System"}),"\n",(0,t.jsx)(e.h3,{id:"end-to-end-integration",children:"End-to-End Integration"}),"\n",(0,t.jsx)(e.p,{children:"Integrate all components into a complete system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class MultimodalVLASystem(nn.Module):\n    \"\"\"\n    Complete multimodal VLA system for humanoid robots.\n    \"\"\"\n\n    def __init__(\n        self,\n        vision_encoder_type: str = 'vit',\n        language_encoder_type: str = 'bert',\n        hidden_dim: int = 768,\n        action_dim: int = 12,\n        use_uncertainty: bool = True,\n        use_hierarchical: bool = True\n    ):\n        super().__init__()\n\n        # Encoders\n        from ..vla_paradigm import VisionEncoder, LanguageEncoder\n\n        self.vision_encoder = VisionEncoder(\n            encoder_type=vision_encoder_type,\n            output_dim=hidden_dim\n        )\n\n        self.language_encoder = LanguageEncoder(\n            model_type=language_encoder_type,\n            output_dim=hidden_dim\n        )\n\n        self.proprio_encoder = nn.Sequential(\n            nn.Linear(64, 256),  # 64-dim proprioceptive input\n            nn.ReLU(),\n            nn.Linear(256, hidden_dim)\n        )\n\n        # Temporal alignment\n        self.temporal_aligner = TemporalAligner()\n\n        # Fusion modules\n        if use_hierarchical:\n            self.fusion = HierarchicalMultimodalFusion(hidden_dim)\n        elif use_uncertainty:\n            self.fusion = UncertaintyAwareFusion(hidden_dim)\n        else:\n            self.fusion = MultimodalTransformer(hidden_dim)\n\n        # Action decoder\n        self.action_decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n\n    def forward(\n        self,\n        images: torch.Tensor,\n        text_instructions: List[str],\n        proprio_states: torch.Tensor,\n        timestamps: Optional[torch.Tensor] = None\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass through multimodal VLA system.\n\n        Args:\n            images: [batch, 3, H, W]\n            text_instructions: List of text strings\n            proprio_states: [batch, 64] - joint positions, velocities, etc.\n            timestamps: Optional temporal information\n\n        Returns:\n            Dictionary with actions and intermediate representations\n        \"\"\"\n        # Encode each modality\n        vision_features = self.vision_encoder(images)\n        language_features = self.language_encoder(text_instructions)\n        proprio_features = self.proprio_encoder(proprio_states)\n\n        # Temporal alignment if timestamps provided\n        if timestamps is not None:\n            # Align features temporally\n            # (Implementation would depend on specific temporal alignment strategy)\n            pass\n\n        # Fuse modalities\n        if isinstance(self.fusion, HierarchicalMultimodalFusion):\n            # Need to add sequence dimension for hierarchical fusion\n            vision_seq = vision_features.unsqueeze(1)\n            language_seq = language_features.unsqueeze(1)\n            proprio_seq = proprio_features.unsqueeze(1)\n\n            fusion_result = self.fusion(vision_seq, language_seq, proprio_seq)\n            fused_features = fusion_result['high_level']\n\n        elif isinstance(self.fusion, UncertaintyAwareFusion):\n            fusion_result = self.fusion(\n                vision_features,\n                language_features,\n                proprio_features\n            )\n            fused_features = fusion_result['fused']\n\n        else:\n            # MultimodalTransformer\n            fusion_result = self.fusion(\n                vision_features=vision_features.unsqueeze(1),\n                language_features=language_features.unsqueeze(1),\n                proprio_features=proprio_features.unsqueeze(1)\n            )\n            fused_features = fusion_result['pooled']\n\n        # Decode actions\n        actions = self.action_decoder(fused_features)\n\n        return {\n            'actions': actions,\n            'fused_features': fused_features,\n            'vision_features': vision_features,\n            'language_features': language_features,\n            'proprio_features': proprio_features,\n            'fusion_details': fusion_result\n        }\n\n    def predict_action(\n        self,\n        image: torch.Tensor,\n        text_instruction: str,\n        proprio_state: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict single action from inputs.\n\n        Args:\n            image: [3, H, W]\n            text_instruction: Text string\n            proprio_state: [64]\n\n        Returns:\n            Predicted action [action_dim]\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            # Add batch dimension\n            images = image.unsqueeze(0)\n            text_instructions = [text_instruction]\n            proprio_states = proprio_state.unsqueeze(0)\n\n            # Forward pass\n            outputs = self.forward(images, text_instructions, proprio_states)\n\n            # Extract action\n            action = outputs['actions'][0]\n\n        return action\n"})}),"\n",(0,t.jsx)(e.h2,{id:"training-strategies",children:"Training Strategies"}),"\n",(0,t.jsx)(e.h3,{id:"multi-task-learning-for-multimodal-systems",children:"Multi-Task Learning for Multimodal Systems"}),"\n",(0,t.jsx)(e.p,{children:"Train on multiple tasks simultaneously:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class MultiTaskTrainer:\n    \"\"\"\n    Trainer for multimodal VLA with multiple tasks.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: MultimodalVLASystem,\n        learning_rate: float = 1e-4,\n        task_weights: Optional[Dict[str, float]] = None\n    ):\n        self.model = model\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=0.01\n        )\n\n        # Task-specific loss weights\n        self.task_weights = task_weights or {\n            'action_prediction': 1.0,\n            'vision_reconstruction': 0.1,\n            'language_grounding': 0.5\n        }\n\n    def compute_multi_task_loss(\n        self,\n        outputs: Dict[str, torch.Tensor],\n        targets: Dict[str, torch.Tensor]\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute multi-task loss.\n\n        Args:\n            outputs: Model outputs\n            targets: Ground truth targets\n\n        Returns:\n            Dictionary of losses\n        \"\"\"\n        losses = {}\n\n        # Action prediction loss\n        if 'actions' in outputs and 'actions' in targets:\n            action_loss = nn.functional.mse_loss(\n                outputs['actions'],\n                targets['actions']\n            )\n            losses['action_prediction'] = action_loss\n\n        # Vision reconstruction loss (optional auxiliary task)\n        if 'reconstructed_image' in outputs and 'image' in targets:\n            recon_loss = nn.functional.mse_loss(\n                outputs['reconstructed_image'],\n                targets['image']\n            )\n            losses['vision_reconstruction'] = recon_loss\n\n        # Language grounding loss (optional auxiliary task)\n        if 'grounding_logits' in outputs and 'grounding_labels' in targets:\n            grounding_loss = nn.functional.cross_entropy(\n                outputs['grounding_logits'],\n                targets['grounding_labels']\n            )\n            losses['language_grounding'] = grounding_loss\n\n        # Weighted total loss\n        total_loss = sum(\n            self.task_weights.get(task, 1.0) * loss\n            for task, loss in losses.items()\n        )\n        losses['total'] = total_loss\n\n        return losses\n\n    def train_step(\n        self,\n        batch: Dict[str, torch.Tensor]\n    ) -> Dict[str, float]:\n        \"\"\"Execute one training step.\"\"\"\n        self.model.train()\n\n        # Forward pass\n        outputs = self.model(\n            images=batch['images'],\n            text_instructions=batch['text_instructions'],\n            proprio_states=batch['proprio_states']\n        )\n\n        # Compute losses\n        losses = self.compute_multi_task_loss(outputs, batch)\n\n        # Backward pass\n        self.optimizer.zero_grad()\n        losses['total'].backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n        # Return scalar losses\n        return {k: v.item() for k, v in losses.items()}\n"})}),"\n",(0,t.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Modality Synchronization"}),": Ensure temporal alignment of different sensor streams"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Uncertainty Quantification"}),": Model and propagate uncertainty through fusion"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Processing"}),": Fuse at multiple levels of abstraction"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Weighting"}),": Learn to weight modalities based on reliability"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-Task Learning"}),": Train on auxiliary tasks to improve representations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robustness"}),": Handle missing or corrupted modalities gracefully"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Interpretability"}),": Visualize attention and modality contributions"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Multimodal integration enables humanoid robots to leverage diverse sensory inputs for comprehensive scene understanding and robust decision-making, creating systems that can operate effectively in complex, dynamic environments."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(u,{...n})}):u(n)}}}]);