"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[3917],{8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}},8680(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var i=t(4848),a=t(8453);const s={},o="Capstone System Design",r={id:"capstone/system-design",title:"Capstone System Design",description:"Overview",source:"@site/docs/capstone/system-design.mdx",sourceDirName:"capstone",slug:"/capstone/system-design",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/capstone/system-design",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/capstone/system-design.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Capstone Project: Building a Complete Humanoid Robot System",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/capstone/"},next:{title:"Capstone Implementation Guide",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/capstone/implementation-guide"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture Principles",id:"system-architecture-principles",level:2},{value:"1. Modularity",id:"1-modularity",level:3},{value:"2. Scalability",id:"2-scalability",level:3},{value:"3. Reliability",id:"3-reliability",level:3},{value:"4. Real-Time Performance",id:"4-real-time-performance",level:3},{value:"High-Level Architecture",id:"high-level-architecture",level:2},{value:"Layered Architecture Diagram",id:"layered-architecture-diagram",level:3},{value:"Component Design",id:"component-design",level:2},{value:"1. Hardware Abstraction Layer",id:"1-hardware-abstraction-layer",level:3},{value:"2. Perception System",id:"2-perception-system",level:3},{value:"3. VLA Integration",id:"3-vla-integration",level:3},{value:"4. Navigation System",id:"4-navigation-system",level:3},{value:"Data Flow Design",id:"data-flow-design",level:2},{value:"Message Flow Diagram",id:"message-flow-diagram",level:3},{value:"ROS 2 Topic Structure",id:"ros-2-topic-structure",level:3},{value:"State Machine Design",id:"state-machine-design",level:2},{value:"High-Level Task State Machine",id:"high-level-task-state-machine",level:3},{value:"Performance Budgets",id:"performance-budgets",level:2},{value:"Latency Budget",id:"latency-budget",level:3},{value:"Computational Budget",id:"computational-budget",level:3},{value:"Safety Architecture",id:"safety-architecture",level:2},{value:"Safety Layers",id:"safety-layers",level:3},{value:"Configuration Management",id:"configuration-management",level:2},{value:"Configuration Hierarchy",id:"configuration-hierarchy",level:3},{value:"Design Checklist",id:"design-checklist",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"capstone-system-design",children:"Capstone System Design"}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This chapter guides you through designing the system architecture for your capstone humanoid robot project. A well-designed architecture is crucial for creating a maintainable, scalable, and robust system that integrates perception, planning, and control."}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture-principles",children:"System Architecture Principles"}),"\n",(0,i.jsx)(n.h3,{id:"1-modularity",children:"1. Modularity"}),"\n",(0,i.jsx)(n.p,{children:"Design independent, reusable components with clear interfaces:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Separate concerns (perception, planning, control)"}),"\n",(0,i.jsx)(n.li,{children:"Use ROS 2 nodes for component isolation"}),"\n",(0,i.jsx)(n.li,{children:"Define clear message interfaces between modules"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-scalability",children:"2. Scalability"}),"\n",(0,i.jsx)(n.p,{children:"Build systems that can grow in capability:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Support adding new sensors and actuators"}),"\n",(0,i.jsx)(n.li,{children:"Allow integration of new AI models"}),"\n",(0,i.jsx)(n.li,{children:"Enable expansion to new tasks"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-reliability",children:"3. Reliability"}),"\n",(0,i.jsx)(n.p,{children:"Ensure robust operation in real-world conditions:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement error handling and recovery"}),"\n",(0,i.jsx)(n.li,{children:"Add redundancy for critical components"}),"\n",(0,i.jsx)(n.li,{children:"Monitor system health continuously"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-real-time-performance",children:"4. Real-Time Performance"}),"\n",(0,i.jsx)(n.p,{children:"Meet timing constraints for robot control:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Prioritize time-critical components"}),"\n",(0,i.jsx)(n.li,{children:"Optimize computation and communication"}),"\n",(0,i.jsx)(n.li,{children:"Monitor and enforce latency budgets"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"layered-architecture-diagram",children:"Layered Architecture Diagram"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    MISSION CONTROL LAYER                         \u2502\n\u2502  \u2022 High-level task planning                                     \u2502\n\u2502  \u2022 Goal management                                              \u2502\n\u2502  \u2022 Human interface                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 COGNITIVE LAYER (VLA)                            \u2502\n\u2502  \u2022 Vision-Language-Action processing                            \u2502\n\u2502  \u2022 Intent understanding                                         \u2502\n\u2502  \u2022 Visual grounding                                             \u2502\n\u2502  \u2022 Action planning                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              PERCEPTION & MAPPING LAYER                          \u2502\n\u2502  \u2022 Visual SLAM                                                   \u2502\n\u2502  \u2022 Object detection                                             \u2502\n\u2502  \u2022 Scene understanding                                          \u2502\n\u2502  \u2022 Map management                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              PLANNING & NAVIGATION LAYER                         \u2502\n\u2502  \u2022 Path planning                                                \u2502\n\u2502  \u2022 Footstep planning                                            \u2502\n\u2502  \u2022 Motion planning                                              \u2502\n\u2502  \u2022 Collision avoidance                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CONTROL LAYER                                  \u2502\n\u2502  \u2022 Balance control                                              \u2502\n\u2502  \u2022 Joint control                                                \u2502\n\u2502  \u2022 Trajectory tracking                                          \u2502\n\u2502  \u2022 Safety monitoring                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 HARDWARE ABSTRACTION LAYER                       \u2502\n\u2502  \u2022 Sensor drivers                                               \u2502\n\u2502  \u2022 Actuator drivers                                             \u2502\n\u2502  \u2022 Communication interfaces                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"component-design",children:"Component Design"}),"\n",(0,i.jsx)(n.h3,{id:"1-hardware-abstraction-layer",children:"1. Hardware Abstraction Layer"}),"\n",(0,i.jsx)(n.p,{children:"Design interfaces for hardware independence:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Hardware abstraction interface\nfrom abc import ABC, abstractmethod\n\nclass RobotHardwareInterface(ABC):\n    """Abstract interface for robot hardware."""\n\n    @abstractmethod\n    def get_joint_states(self):\n        """Get current joint positions and velocities."""\n        pass\n\n    @abstractmethod\n    def send_joint_commands(self, positions):\n        """Send joint position commands."""\n        pass\n\n    @abstractmethod\n    def get_imu_data(self):\n        """Get IMU measurements."""\n        pass\n\n    @abstractmethod\n    def get_camera_image(self):\n        """Get camera image."""\n        pass\n\n    @abstractmethod\n    def emergency_stop(self):\n        """Execute emergency stop."""\n        pass\n\n\nclass SimulationHardware(RobotHardwareInterface):\n    """Hardware interface for Gazebo simulation."""\n\n    def __init__(self):\n        # Initialize ROS 2 interfaces for Gazebo\n        pass\n\n    def get_joint_states(self):\n        # Get from Gazebo simulation\n        pass\n\n    # Implement other methods...\n\n\nclass PhysicalHardware(RobotHardwareInterface):\n    """Hardware interface for physical robot."""\n\n    def __init__(self):\n        # Initialize drivers for physical hardware\n        pass\n\n    def get_joint_states(self):\n        # Get from actual sensors\n        pass\n\n    # Implement other methods...\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-perception-system",children:"2. Perception System"}),"\n",(0,i.jsx)(n.p,{children:"Design modular perception pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# perception_architecture.yaml\nperception_system:\n  components:\n    - name: camera_processing\n      type: sensor_processor\n      inputs:\n        - /camera/rgb/image_raw\n      outputs:\n        - /perception/processed_image\n      parameters:\n        resolution: [640, 480]\n        fps: 30\n\n    - name: object_detection\n      type: ai_inference\n      inputs:\n        - /perception/processed_image\n      outputs:\n        - /perception/detected_objects\n      model:\n        type: yolov8\n        weights: models/object_detector.pt\n        confidence_threshold: 0.5\n\n    - name: visual_slam\n      type: slam\n      inputs:\n        - /camera/rgb/image_raw\n        - /imu/data\n      outputs:\n        - /perception/odometry\n        - /perception/map\n      parameters:\n        map_resolution: 0.05\n        enable_loop_closure: true\n\n    - name: scene_understanding\n      type: semantic_segmentation\n      inputs:\n        - /perception/processed_image\n      outputs:\n        - /perception/semantic_map\n      model:\n        type: segformer\n        weights: models/segmentation.pt\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-vla-integration",children:"3. VLA Integration"}),"\n",(0,i.jsx)(n.p,{children:"Design vision-language-action pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# VLA system architecture\nclass VLASystem:\n    """Complete VLA system for humanoid robot."""\n\n    def __init__(self, config):\n        # Language processing\n        self.intent_recognizer = IntentRecognizer()\n        self.semantic_parser = SemanticParser()\n\n        # Vision processing\n        self.vision_grounder = VisionGrounder()\n        self.object_detector = ObjectDetector()\n\n        # Multimodal fusion\n        self.multimodal_fusion = MultimodalFusion()\n\n        # Action generation\n        self.action_planner = ActionPlanner()\n        self.motion_generator = MotionGenerator()\n\n        # Integration\n        self.state_machine = TaskStateMachine()\n        self.safety_monitor = SafetyMonitor()\n\n    def process_command(self, text_command, image, robot_state):\n        """Process natural language command."""\n\n        # 1. Understand intent\n        intent = self.intent_recognizer.classify(text_command)\n        parsed_command = self.semantic_parser.parse(text_command)\n\n        # 2. Ground in visual scene\n        grounded_objects = self.vision_grounder.ground(\n            image, parsed_command\n        )\n\n        # 3. Fuse multimodal information\n        fused_representation = self.multimodal_fusion.fuse(\n            vision=grounded_objects,\n            language=parsed_command,\n            proprio=robot_state\n        )\n\n        # 4. Generate action plan\n        action_sequence = self.action_planner.plan(\n            intent, fused_representation\n        )\n\n        # 5. Generate motion\n        trajectories = self.motion_generator.generate(\n            action_sequence, robot_state\n        )\n\n        # 6. Safety check\n        safe_trajectories = self.safety_monitor.validate(\n            trajectories, robot_state\n        )\n\n        return safe_trajectories\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4-navigation-system",children:"4. Navigation System"}),"\n",(0,i.jsx)(n.p,{children:"Design hierarchical navigation architecture:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Navigation System Architecture:\n\u251c\u2500\u2500 Global Planner\n\u2502   \u251c\u2500\u2500 Map representation (occupancy grid)\n\u2502   \u251c\u2500\u2500 Path finding (A*, RRT)\n\u2502   \u2514\u2500\u2500 Waypoint generation\n\u251c\u2500\u2500 Local Planner\n\u2502   \u251c\u2500\u2500 Dynamic obstacle avoidance\n\u2502   \u251c\u2500\u2500 Velocity planning\n\u2502   \u2514\u2500\u2500 Replanning on failure\n\u251c\u2500\u2500 Footstep Planner\n\u2502   \u251c\u2500\u2500 Balance-aware stepping\n\u2502   \u251c\u2500\u2500 Terrain adaptation\n\u2502   \u2514\u2500\u2500 Step sequence generation\n\u251c\u2500\u2500 Balance Controller\n\u2502   \u251c\u2500\u2500 ZMP control\n\u2502   \u251c\u2500\u2500 CoM management\n\u2502   \u2514\u2500\u2500 Stabilization\n\u2514\u2500\u2500 Motion Executor\n    \u251c\u2500\u2500 Trajectory tracking\n    \u251c\u2500\u2500 Joint control\n    \u2514\u2500\u2500 Feedback control\n"})}),"\n",(0,i.jsx)(n.h2,{id:"data-flow-design",children:"Data Flow Design"}),"\n",(0,i.jsx)(n.h3,{id:"message-flow-diagram",children:"Message Flow Diagram"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Sensors \u2192 Perception \u2192 Cognitive \u2192 Planning \u2192 Control \u2192 Actuators\n   \u2502          \u2502           \u2502           \u2502          \u2502          \u2502\n   \u2502          \u2193           \u2193           \u2193          \u2193          \u2502\n   \u2502      [Object     [Intent     [Path      [Balance      \u2502\n   \u2502       Detection]  Recognition] Planning]  Control]     \u2502\n   \u2502          \u2502           \u2502           \u2502          \u2502          \u2502\n   \u2502          \u2193           \u2193           \u2193          \u2193          \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2192 State Estimation \u2190\u2500\u2500\u2500 Feedback \u2190\u2500\u2500\u2500\u2500\u2518          \u2502\n                 \u2502                                           \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Safety Monitor \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-topic-structure",children:"ROS 2 Topic Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Topic organization for capstone system\ntopics:\n  # Sensors (raw data)\n  sensors:\n    - /camera/rgb/image_raw\n    - /camera/depth/image_raw\n    - /imu/data\n    - /joint_states\n    - /force_torque/left_foot\n    - /force_torque/right_foot\n\n  # Perception (processed information)\n  perception:\n    - /perception/objects\n    - /perception/semantic_map\n    - /perception/odometry\n    - /perception/point_cloud\n\n  # VLA (cognitive processing)\n  vla:\n    - /vla/command (String)\n    - /vla/intent (Intent)\n    - /vla/grounded_objects (ObjectArray)\n    - /vla/action_plan (ActionSequence)\n\n  # Navigation\n  navigation:\n    - /navigation/global_plan (Path)\n    - /navigation/local_plan (Path)\n    - /navigation/footsteps (FootstepArray)\n    - /navigation/goal (PoseStamped)\n\n  # Control\n  control:\n    - /control/joint_commands (JointTrajectory)\n    - /control/balance_state (BalanceState)\n    - /control/emergency_stop (Bool)\n\n  # System\n  system:\n    - /diagnostics\n    - /system/status\n    - /safety/violations\n"})}),"\n",(0,i.jsx)(n.h2,{id:"state-machine-design",children:"State Machine Design"}),"\n",(0,i.jsx)(n.h3,{id:"high-level-task-state-machine",children:"High-Level Task State Machine"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from enum import Enum\nfrom transitions import Machine\n\nclass RobotState(Enum):\n    IDLE = \"idle\"\n    PROCESSING_COMMAND = \"processing_command\"\n    NAVIGATING = \"navigating\"\n    MANIPULATING = \"manipulating\"\n    RECOVERING = \"recovering\"\n    EMERGENCY_STOP = \"emergency_stop\"\n\nclass TaskStateMachine:\n    \"\"\"State machine for high-level task control.\"\"\"\n\n    states = [s.value for s in RobotState]\n\n    transitions = [\n        # From IDLE\n        {'trigger': 'receive_command', 'source': 'idle',\n         'dest': 'processing_command'},\n\n        # From PROCESSING_COMMAND\n        {'trigger': 'navigate_required', 'source': 'processing_command',\n         'dest': 'navigating'},\n        {'trigger': 'manipulate_required', 'source': 'processing_command',\n         'dest': 'manipulating'},\n\n        # From NAVIGATING\n        {'trigger': 'navigation_complete', 'source': 'navigating',\n         'dest': 'manipulating'},\n        {'trigger': 'navigation_failed', 'source': 'navigating',\n         'dest': 'recovering'},\n        {'trigger': 'goal_reached', 'source': 'navigating',\n         'dest': 'idle'},\n\n        # From MANIPULATING\n        {'trigger': 'manipulation_complete', 'source': 'manipulating',\n         'dest': 'idle'},\n        {'trigger': 'manipulation_failed', 'source': 'manipulating',\n         'dest': 'recovering'},\n\n        # From RECOVERING\n        {'trigger': 'recovery_success', 'source': 'recovering',\n         'dest': 'idle'},\n        {'trigger': 'recovery_failed', 'source': 'recovering',\n         'dest': 'emergency_stop'},\n\n        # Emergency stop (from any state)\n        {'trigger': 'emergency', 'source': '*',\n         'dest': 'emergency_stop'},\n\n        # Reset from emergency\n        {'trigger': 'reset', 'source': 'emergency_stop',\n         'dest': 'idle'}\n    ]\n\n    def __init__(self):\n        self.machine = Machine(\n            model=self,\n            states=TaskStateMachine.states,\n            transitions=TaskStateMachine.transitions,\n            initial='idle'\n        )\n\n    def on_enter_processing_command(self):\n        \"\"\"Actions when entering processing_command state.\"\"\"\n        print(\"Processing new command...\")\n\n    def on_enter_navigating(self):\n        \"\"\"Actions when entering navigating state.\"\"\"\n        print(\"Starting navigation...\")\n\n    # ... other state entry/exit handlers\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-budgets",children:"Performance Budgets"}),"\n",(0,i.jsx)(n.h3,{id:"latency-budget",children:"Latency Budget"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Latency requirements for real-time operation\nlatency_budgets:\n  perception:\n    camera_processing: 33ms  # 30 FPS\n    object_detection: 50ms\n    slam_update: 100ms\n\n  vla:\n    intent_recognition: 100ms\n    vision_grounding: 150ms\n    action_planning: 200ms\n\n  control:\n    balance_control: 10ms   # 100 Hz\n    joint_control: 5ms      # 200 Hz\n    safety_check: 5ms\n\n  total_command_to_action: 500ms  # Maximum acceptable delay\n"})}),"\n",(0,i.jsx)(n.h3,{id:"computational-budget",children:"Computational Budget"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Resource allocation for Jetson AGX Orin (64GB)\ncompute_budget:\n  gpu:\n    vla_inference: 40%\n    object_detection: 30%\n    slam: 20%\n    other: 10%\n\n  cpu:\n    control: 30%\n    planning: 25%\n    communication: 20%\n    monitoring: 15%\n    other: 10%\n\n  memory:\n    vla_model: 8GB\n    slam_map: 4GB\n    perception_buffers: 2GB\n    system: 2GB\n    reserve: 4GB\n"})}),"\n",(0,i.jsx)(n.h2,{id:"safety-architecture",children:"Safety Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"safety-layers",children:"Safety Layers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Safety Layer 1: Hardware Emergency Stop\n  \u251c\u2500\u2500 Physical e-stop button\n  \u251c\u2500\u2500 Motor power cutoff\n  \u2514\u2500\u2500 Brake engagement\n\nSafety Layer 2: Control-Level Safety\n  \u251c\u2500\u2500 Joint limit enforcement\n  \u251c\u2500\u2500 Velocity limits\n  \u251c\u2500\u2500 Torque limits\n  \u2514\u2500\u2500 Collision detection\n\nSafety Layer 3: Planning-Level Safety\n  \u251c\u2500\u2500 Workspace boundaries\n  \u251c\u2500\u2500 Balance verification\n  \u251c\u2500\u2500 Path safety checking\n  \u2514\u2500\u2500 Obstacle avoidance\n\nSafety Layer 4: Cognitive-Level Safety\n  \u251c\u2500\u2500 Command validation\n  \u251c\u2500\u2500 Intent verification\n  \u251c\u2500\u2500 Action feasibility check\n  \u2514\u2500\u2500 Human approval for risky actions\n\nSafety Layer 5: System-Level Monitoring\n  \u251c\u2500\u2500 Watchdog timers\n  \u251c\u2500\u2500 Health monitoring\n  \u251c\u2500\u2500 Error detection\n  \u2514\u2500\u2500 Graceful degradation\n"})}),"\n",(0,i.jsx)(n.h2,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,i.jsx)(n.h3,{id:"configuration-hierarchy",children:"Configuration Hierarchy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Master configuration file\nrobot_config:\n  hardware:\n    platform: "simulation"  # or "nao", "pepper", "custom"\n    joints: 12\n    cameras: 2\n    sensors:\n      - type: "imu"\n        topic: "/imu/data"\n      - type: "camera"\n        topic: "/camera/rgb/image_raw"\n\n  perception:\n    slam:\n      enable: true\n      config: "config/slam_config.yaml"\n    object_detection:\n      enable: true\n      model: "models/yolov8n.pt"\n      confidence: 0.5\n\n  vla:\n    enable: true\n    model_path: "models/vla_model.pt"\n    optimization: "tensorrt"\n    precision: "fp16"\n\n  navigation:\n    planner: "rrt_star"\n    local_planner: "dwa"\n    footstep_planner: "balance_aware"\n\n  control:\n    balance_controller: "zmp"\n    joint_controller: "position"\n    control_frequency: 100\n\n  safety:\n    enable_all_checks: true\n    emergency_stop_enabled: true\n    workspace_limits:\n      x: [-1.0, 1.0]\n      y: [-1.0, 1.0]\n      z: [0.0, 2.0]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"design-checklist",children:"Design Checklist"}),"\n",(0,i.jsx)(n.p,{children:"Before finalizing your design, verify:"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All required capabilities are addressed"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Component interfaces are well-defined"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 topics and services are organized logically"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Latency budgets are realistic and achievable"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Computational resources are allocated appropriately"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Safety mechanisms are comprehensive"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","State transitions are clearly defined"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configuration is externalized and manageable"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System can be tested incrementally"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design supports future extensions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"With your system design complete, proceed to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implementation Guide"}),": Build your system step-by-step"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Edge Deployment"}),": Optimize and deploy to hardware"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"A well-thought-out design is the foundation of a successful capstone project!"})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);