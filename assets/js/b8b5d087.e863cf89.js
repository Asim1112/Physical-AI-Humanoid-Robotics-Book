"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[6313],{7889(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>d});var s=t(4848),i=t(8453);const o={},r="Edge Deployment for Humanoid Robots",a={id:"capstone/edge-deployment",title:"Edge Deployment for Humanoid Robots",description:"Overview",source:"@site/docs/capstone/edge-deployment.mdx",sourceDirName:"capstone",slug:"/capstone/edge-deployment",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/capstone/edge-deployment",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/capstone/edge-deployment.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Capstone Implementation Guide",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/capstone/implementation-guide"},next:{title:"References and Resources",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/references"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"Target Platforms",id:"target-platforms",level:2},{value:"NVIDIA Jetson Family",id:"nvidia-jetson-family",level:3},{value:"Raspberry Pi",id:"raspberry-pi",level:3},{value:"Industrial Computers",id:"industrial-computers",level:3},{value:"Pre-Deployment Checklist",id:"pre-deployment-checklist",level:2},{value:"Model Optimization",id:"model-optimization",level:2},{value:"1. TensorRT Optimization",id:"1-tensorrt-optimization",level:3},{value:"2. Model Quantization",id:"2-model-quantization",level:3},{value:"3. Model Pruning",id:"3-model-pruning",level:3},{value:"System Configuration for Jetson",id:"system-configuration-for-jetson",level:2},{value:"1. Power Mode Configuration",id:"1-power-mode-configuration",level:3},{value:"2. Docker Deployment",id:"2-docker-deployment",level:3},{value:"3. ROS 2 Configuration for Edge",id:"3-ros-2-configuration-for-edge",level:3},{value:"Resource Monitoring",id:"resource-monitoring",level:2},{value:"1. System Monitor Node",id:"1-system-monitor-node",level:3},{value:"2. Performance Dashboard",id:"2-performance-dashboard",level:3},{value:"Deployment Workflow",id:"deployment-workflow",level:2},{value:"1. Pre-Deployment Testing",id:"1-pre-deployment-testing",level:3},{value:"2. Deployment Script",id:"2-deployment-script",level:3},{value:"3. Auto-Start on Boot",id:"3-auto-start-on-boot",level:3},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:2},{value:"1. Memory Management",id:"1-memory-management",level:3},{value:"2. Inference Optimization",id:"2-inference-optimization",level:3},{value:"3. Multi-Threading",id:"3-multi-threading",level:3},{value:"Troubleshooting Edge Deployment",id:"troubleshooting-edge-deployment",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Production Deployment Checklist",id:"production-deployment-checklist",level:2},{value:"Remote Management",id:"remote-management",level:2},{value:"SSH Configuration",id:"ssh-configuration",level:3},{value:"Remote Monitoring",id:"remote-monitoring",level:3},{value:"Over-The-Air (OTA) Updates",id:"over-the-air-ota-updates",level:2},{value:"Best Practices",id:"best-practices",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"edge-deployment-for-humanoid-robots",children:"Edge Deployment for Humanoid Robots"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Deploying your humanoid robot system on edge devices (Jetson, RPi, embedded systems) requires optimization for resource-constrained environments. This guide covers model optimization, system configuration, and deployment strategies for production-ready humanoid robots."}),"\n",(0,s.jsx)(n.h2,{id:"target-platforms",children:"Target Platforms"}),"\n",(0,s.jsx)(n.h3,{id:"nvidia-jetson-family",children:"NVIDIA Jetson Family"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson AGX Orin"}),": 275 TOPS AI, 64GB RAM (recommended for VLA)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson Orin NX"}),": 100 TOPS AI, 16GB RAM (good balance)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson Orin Nano"}),": 40 TOPS AI, 8GB RAM (budget option)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson Xavier NX"}),": 21 TOPS AI, 8GB RAM (previous gen)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"raspberry-pi",children:"Raspberry Pi"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Raspberry Pi 5"}),": 8GB RAM (basic perception)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Raspberry Pi 4"}),": 8GB RAM (limited AI capabilities)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"industrial-computers",children:"Industrial Computers"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intel NUC with dGPU"}),": Flexible, powerful"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom x86 with NVIDIA GPU"}),": Maximum performance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"pre-deployment-checklist",children:"Pre-Deployment Checklist"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All components tested in simulation"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance profiling completed"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Models optimized for inference"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Safety mechanisms validated"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Documentation complete"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Backup and recovery procedures in place"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitoring and logging configured"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-tensorrt-optimization",children:"1. TensorRT Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Convert PyTorch models to TensorRT for maximum performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# optimize_models.py\nimport torch\nimport torch_tensorrt\nfrom pathlib import Path\n\ndef optimize_vla_model():\n    """Optimize VLA model for Jetson deployment."""\n\n    # Load PyTorch model\n    model = torch.jit.load(\'models/vla_model.pt\')\n    model.eval()\n\n    # Define input specifications for Jetson\n    inputs = [\n        torch_tensorrt.Input(\n            min_shape=[1, 3, 224, 224],\n            opt_shape=[1, 3, 224, 224],\n            max_shape=[1, 3, 224, 224],\n            dtype=torch.float32\n        ),\n        torch_tensorrt.Input(\n            min_shape=[1, 64],\n            opt_shape=[1, 64],\n            max_shape=[1, 64],\n            dtype=torch.float32\n        )\n    ]\n\n    # Compile with TensorRT\n    trt_model = torch_tensorrt.compile(\n        model,\n        inputs=inputs,\n        enabled_precisions={torch.float16},  # FP16 for Jetson\n        workspace_size=1 << 30,  # 1GB workspace\n        truncate_long_and_double=True\n    )\n\n    # Save optimized model\n    torch.jit.save(trt_model, \'models/vla_model_trt.pt\')\n    print("VLA model optimized for TensorRT")\n\ndef optimize_object_detector():\n    """Optimize YOLOv8 for edge deployment."""\n    from ultralytics import YOLO\n\n    # Load model\n    model = YOLO(\'yolov8n.pt\')  # Use nano version for edge\n\n    # Export to TensorRT\n    model.export(\n        format=\'engine\',\n        device=0,\n        half=True,  # FP16\n        workspace=4,  # 4GB\n        simplify=True\n    )\n    print("Object detector optimized for TensorRT")\n\nif __name__ == \'__main__\':\n    optimize_vla_model()\n    optimize_object_detector()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-model-quantization",children:"2. Model Quantization"}),"\n",(0,s.jsx)(n.p,{children:"Reduce model size and improve inference speed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# quantize_models.py\nimport torch\nfrom torch.quantization import quantize_dynamic\n\ndef quantize_language_model():\n    """Quantize BERT model to INT8."""\n\n    model = torch.jit.load(\'models/language_encoder.pt\')\n\n    # Dynamic quantization (for LSTM/Linear layers)\n    quantized_model = quantize_dynamic(\n        model,\n        {torch.nn.Linear},\n        dtype=torch.qint8\n    )\n\n    # Save quantized model\n    torch.jit.save(quantized_model, \'models/language_encoder_int8.pt\')\n\n    # Compare sizes\n    original_size = Path(\'models/language_encoder.pt\').stat().st_size / 1024**2\n    quantized_size = Path(\'models/language_encoder_int8.pt\').stat().st_size / 1024**2\n\n    print(f"Original: {original_size:.1f} MB")\n    print(f"Quantized: {quantized_size:.1f} MB")\n    print(f"Reduction: {(1 - quantized_size/original_size)*100:.1f}%")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-model-pruning",children:"3. Model Pruning"}),"\n",(0,s.jsx)(n.p,{children:"Remove redundant parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# prune_models.py\nimport torch\nimport torch.nn.utils.prune as prune\n\ndef prune_vla_model(model, amount=0.3):\n    """\n    Prune VLA model by removing least important weights.\n\n    Args:\n        model: PyTorch model\n        amount: Fraction of weights to prune (0.3 = 30%)\n    """\n\n    # Prune each linear layer\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            prune.l1_unstructured(module, name=\'weight\', amount=amount)\n            # Make pruning permanent\n            prune.remove(module, \'weight\')\n\n    return model\n'})}),"\n",(0,s.jsx)(n.h2,{id:"system-configuration-for-jetson",children:"System Configuration for Jetson"}),"\n",(0,s.jsx)(n.h3,{id:"1-power-mode-configuration",children:"1. Power Mode Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Maximize performance on Jetson:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Set maximum performance mode\nsudo nvpmodel -m 0\nsudo jetson_clocks\n\n# Verify settings\nsudo nvpmodel -q\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-docker-deployment",children:"2. Docker Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Create Docker container for consistent deployment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile\nFROM nvcr.io/nvidia/l4t-pytorch:r35.2.1-pth2.0-py3\n\n# Install ROS 2\nRUN apt-get update && apt-get install -y \\\n    ros-humble-desktop \\\n    ros-humble-ros2-control \\\n    ros-humble-ros2-controllers \\\n    python3-colcon-common-extensions\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip3 install -r requirements.txt\n\n# Copy application\nCOPY humanoid_capstone/ /workspace/humanoid_capstone/\n\n# Build ROS workspace\nWORKDIR /workspace\nRUN . /opt/ros/humble/setup.sh && colcon build\n\n# Entry point\nCOPY entrypoint.sh /\nRUN chmod +x /entrypoint.sh\nENTRYPOINT ["/entrypoint.sh"]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Build and run:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Build Docker image\ndocker build -t humanoid-robot:latest .\n\n# Run with GPU support\ndocker run --runtime nvidia --network host \\\n    -v /dev:/dev --privileged \\\n    humanoid-robot:latest\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-ros-2-configuration-for-edge",children:"3. ROS 2 Configuration for Edge"}),"\n",(0,s.jsx)(n.p,{children:"Optimize ROS 2 for edge deployment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# config/edge_deployment.yaml\n/**:\n  ros__parameters:\n    # Use intra-process communication\n    use_intra_process_comms: true\n\n    # QoS settings for reliability vs latency\n    qos_overrides:\n      /camera/rgb/image_raw:\n        reliability: best_effort\n        durability: volatile\n        history: keep_last\n        depth: 1\n\n      /joint_states:\n        reliability: reliable\n        durability: volatile\n        history: keep_last\n        depth: 10\n\n    # Reduce update rates for non-critical topics\n    camera_fps: 15  # Reduced from 30\n    slam_update_rate: 5  # Reduced from 10\n    vla_inference_rate: 5  # Reduced from 10\n"})}),"\n",(0,s.jsx)(n.h2,{id:"resource-monitoring",children:"Resource Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"1-system-monitor-node",children:"1. System Monitor Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/monitoring/system_monitor.py\nimport psutil\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32MultiArray\n\nclass SystemMonitor(Node):\n    def __init__(self):\n        super().__init__('system_monitor')\n\n        # Publishers\n        self.stats_pub = self.create_publisher(\n            Float32MultiArray, '/system/stats', 10)\n\n        # Timer (every 5 seconds)\n        self.timer = self.create_timer(5.0, self.publish_stats)\n\n    def publish_stats(self):\n        \"\"\"Publish system statistics.\"\"\"\n\n        stats = Float32MultiArray()\n\n        # CPU usage\n        cpu_percent = psutil.cpu_percent(interval=1)\n\n        # Memory usage\n        mem = psutil.virtual_memory()\n        mem_percent = mem.percent\n\n        # GPU stats (NVIDIA)\n        try:\n            import pynvml\n            pynvml.nvmlInit()\n            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n            gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu\n            gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**3\n        except:\n            gpu_util = 0\n            gpu_mem = 0\n\n        # Temperature\n        try:\n            temps = psutil.sensors_temperatures()\n            temp = temps['thermal'][0].current if 'thermal' in temps else 0\n        except:\n            temp = 0\n\n        stats.data = [cpu_percent, mem_percent, gpu_util, gpu_mem, temp]\n        self.stats_pub.publish(stats)\n\n        # Log warnings\n        if cpu_percent > 90:\n            self.get_logger().warn(f'High CPU usage: {cpu_percent}%')\n        if mem_percent > 90:\n            self.get_logger().warn(f'High memory usage: {mem_percent}%')\n        if temp > 80:\n            self.get_logger().warn(f'High temperature: {temp}\xb0C')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-performance-dashboard",children:"2. Performance Dashboard"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# dashboard/performance_viz.py\nimport dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nimport plotly.graph_objs as go\nimport rclpy\nfrom std_msgs.msg import Float32MultiArray\n\nclass PerformanceDashboard:\n    def __init__(self):\n        self.app = dash.Dash(__name__)\n        self.cpu_data = []\n        self.mem_data = []\n        self.gpu_data = []\n\n        self.setup_layout()\n        self.setup_callbacks()\n\n    def setup_layout(self):\n        self.app.layout = html.Div([\n            html.H1('Humanoid Robot Performance Dashboard'),\n\n            dcc.Graph(id='cpu-graph'),\n            dcc.Graph(id='memory-graph'),\n            dcc.Graph(id='gpu-graph'),\n\n            dcc.Interval(id='interval', interval=1000)  # Update every second\n        ])\n\n    def setup_callbacks(self):\n        @self.app.callback(\n            [Output('cpu-graph', 'figure'),\n             Output('memory-graph', 'figure'),\n             Output('gpu-graph', 'figure')],\n            [Input('interval', 'n_intervals')]\n        )\n        def update_graphs(n):\n            # Create figures for CPU, Memory, GPU\n            cpu_fig = go.Figure(data=[go.Scatter(y=self.cpu_data)])\n            mem_fig = go.Figure(data=[go.Scatter(y=self.mem_data)])\n            gpu_fig = go.Figure(data=[go.Scatter(y=self.gpu_data)])\n\n            return cpu_fig, mem_fig, gpu_fig\n\n    def run(self):\n        self.app.run_server(debug=False, host='0.0.0.0', port=8050)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"deployment-workflow",children:"Deployment Workflow"}),"\n",(0,s.jsx)(n.h3,{id:"1-pre-deployment-testing",children:"1. Pre-Deployment Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# pre_deployment_test.sh\n\necho "Running pre-deployment tests..."\n\n# Build system\ncolcon build --packages-select humanoid_capstone\n\n# Run unit tests\ncolcon test --packages-select humanoid_capstone\n\n# Run integration tests\nros2 launch humanoid_capstone integration_test.launch.py\n\n# Check performance\nros2 run humanoid_capstone performance_benchmark\n\n# Verify safety systems\nros2 run humanoid_capstone safety_test\n\necho "Pre-deployment tests complete!"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-deployment-script",children:"2. Deployment Script"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# deploy_to_jetson.sh\n\nJETSON_IP="192.168.1.100"\nJETSON_USER="nvidia"\n\necho "Deploying to Jetson at $JETSON_IP..."\n\n# Copy files\nrsync -avz --exclude=\'build\' --exclude=\'install\' \\\n    ./ $JETSON_USER@$JETSON_IP:~/humanoid_ws/src/humanoid_capstone/\n\n# SSH and build\nssh $JETSON_USER@$JETSON_IP << \'EOF\'\n    cd ~/humanoid_ws\n    source /opt/ros/humble/setup.bash\n    colcon build --packages-select humanoid_capstone\n    echo "Build complete!"\nEOF\n\necho "Deployment complete!"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-auto-start-on-boot",children:"3. Auto-Start on Boot"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create systemd service\nsudo nano /etc/systemd/system/humanoid-robot.service\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ini",children:'# humanoid-robot.service\n[Unit]\nDescription=Humanoid Robot System\nAfter=network.target\n\n[Service]\nType=simple\nUser=nvidia\nWorkingDirectory=/home/nvidia/humanoid_ws\nExecStart=/bin/bash -c "source /opt/ros/humble/setup.bash && source install/setup.bash && ros2 launch humanoid_capstone full_system.launch.py"\nRestart=on-failure\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n'})}),"\n",(0,s.jsx)(n.p,{children:"Enable service:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo systemctl enable humanoid-robot.service\nsudo systemctl start humanoid-robot.service\nsudo systemctl status humanoid-robot.service\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,s.jsx)(n.h3,{id:"1-memory-management",children:"1. Memory Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Optimize memory usage\nimport gc\nimport torch\n\n# Clear unused memory periodically\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Use gradient checkpointing for large models\nmodel.gradient_checkpointing_enable()\n\n# Limit batch size\nBATCH_SIZE = 1  # Process one at a time on edge\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-inference-optimization",children:"2. Inference Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Use mixed precision\nfrom torch.cuda.amp import autocast\n\n@torch.no_grad()\ndef optimized_inference(model, inputs):\n    with autocast():\n        outputs = model(inputs)\n    return outputs\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-multi-threading",children:"3. Multi-Threading"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Use threading for I/O operations\nimport threading\n\ndef async_image_processing():\n    while running:\n        image = camera.get_frame()\n        processed = process_image(image)\n        queue.put(processed)\n\n# Start image processing in background\nthread = threading.Thread(target=async_image_processing, daemon=True)\nthread.start()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-edge-deployment",children:"Troubleshooting Edge Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": Out of memory errors"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Solution: Monitor and optimize\nsudo tegrastats  # For Jetson\n# Reduce batch sizes, use model pruning\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": Slow inference"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Solution: Use TensorRT, reduce model size\n# Verify GPU utilization\nnvidia-smi\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": ROS 2 nodes crashing"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Solution: Check logs\njournalctl -u humanoid-robot.service -f\n# Increase memory limits in systemd service\n"})}),"\n",(0,s.jsx)(n.h2,{id:"production-deployment-checklist",children:"Production Deployment Checklist"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Models Optimized"}),": All models converted to TensorRT/quantized"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Performance Validated"}),": Meets real-time requirements"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Safety Tested"}),": Emergency stop, joint limits verified"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Monitoring Configured"}),": System metrics logged"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Auto-Start Enabled"}),": System starts on boot"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Backup Strategy"}),": Regular backups configured"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Update Procedure"}),": OTA update mechanism in place"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Documentation"}),": Deployment guide complete"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"User Training"}),": Operators trained on system"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Maintenance Plan"}),": Regular maintenance scheduled"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"remote-management",children:"Remote Management"}),"\n",(0,s.jsx)(n.h3,{id:"ssh-configuration",children:"SSH Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Enable SSH on Jetson\nsudo systemctl enable ssh\nsudo systemctl start ssh\n\n# Set up SSH keys for passwordless access\nssh-copy-id nvidia@jetson-ip\n"})}),"\n",(0,s.jsx)(n.h3,{id:"remote-monitoring",children:"Remote Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Monitor system remotely\nssh nvidia@jetson-ip "sudo tegrastats"\n\n# View logs remotely\nssh nvidia@jetson-ip "journalctl -u humanoid-robot -f"\n\n# Remote restart\nssh nvidia@jetson-ip "sudo systemctl restart humanoid-robot"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"over-the-air-ota-updates",children:"Over-The-Air (OTA) Updates"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# ota_updater.py\nimport subprocess\nimport rclpy\nfrom rclpy.node import Node\n\nclass OTAUpdater(Node):\n    def __init__(self):\n        super().__init__('ota_updater')\n\n    def check_for_updates(self):\n        \"\"\"Check GitHub for new releases.\"\"\"\n        # Implementation\n        pass\n\n    def download_update(self, version):\n        \"\"\"Download update package.\"\"\"\n        # Implementation\n        pass\n\n    def apply_update(self):\n        \"\"\"Apply update and restart system.\"\"\"\n        # Stop current system\n        subprocess.run(['sudo', 'systemctl', 'stop', 'humanoid-robot'])\n\n        # Apply update\n        subprocess.run(['colcon', 'build'])\n\n        # Restart system\n        subprocess.run(['sudo', 'systemctl', 'start', 'humanoid-robot'])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test Extensively"}),": Test on target hardware before deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Continuously"}),": Track performance metrics in production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update Gradually"}),": Roll out updates to test units first"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintain Backups"}),": Keep working versions backed up"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Document Everything"}),": Maintain deployment documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan for Failure"}),": Have recovery procedures ready"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize Iteratively"}),": Continuously improve performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Secure System"}),": Implement proper security measures"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Successful edge deployment requires careful optimization, thorough testing, and continuous monitoring to ensure reliable operation of your humanoid robot system!"})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);