"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[1011],{3451(n,e,t){t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var i=t(4848),s=t(8453);const o={},a="Vision-Language-Action Paradigm",r={id:"module-4-vla/vla-paradigm",title:"Vision-Language-Action Paradigm",description:"Overview",source:"@site/docs/module-4-vla/vla-paradigm.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/vla-paradigm",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vla-paradigm",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-4-vla/vla-paradigm.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 4: Vision-Language-Action (VLA)",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/"},next:{title:"Language and Intent Understanding",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/language-intent"}},d={},l=[{value:"Overview",id:"overview",level:2},{value:"The VLA Architecture",id:"the-vla-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Vision Encoder",id:"vision-encoder",level:3},{value:"Language Encoder",id:"language-encoder",level:3},{value:"Multimodal Fusion",id:"multimodal-fusion",level:3},{value:"Action Decoder",id:"action-decoder",level:3},{value:"Complete VLA Model",id:"complete-vla-model",level:2},{value:"Training VLA Models",id:"training-vla-models",level:2},{value:"Dataset Requirements",id:"dataset-requirements",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"VLA Model Variants",id:"vla-model-variants",level:2},{value:"RT-1 Style Architecture",id:"rt-1-style-architecture",level:3},{value:"RT-2 with Vision-Language Pre-training",id:"rt-2-with-vision-language-pre-training",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Model Optimization",id:"model-optimization",level:3},{value:"Best Practices",id:"best-practices",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"vision-language-action-paradigm",children:"Vision-Language-Action Paradigm"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"The Vision-Language-Action (VLA) paradigm represents a fundamental shift in how we design and train robotic systems. Rather than building separate pipelines for vision, language understanding, and control, VLA models create an end-to-end system that directly maps from multimodal inputs (images and text) to robot actions. This chapter explores the architecture, principles, and implementation of VLA models for humanoid robotics."}),"\n",(0,i.jsx)(e.h2,{id:"the-vla-architecture",children:"The VLA Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,i.jsx)(e.p,{children:"A typical VLA model consists of three main components that work together in a unified architecture:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         VLA MODEL                                 \u2502\n\u2502                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  VISION        \u2502  \u2502  LANGUAGE      \u2502  \u2502  ACTION        \u2502    \u2502\n\u2502  \u2502  ENCODER       \u2502  \u2502  ENCODER       \u2502  \u2502  DECODER       \u2502    \u2502\n\u2502  \u2502                \u2502  \u2502                \u2502  \u2502                \u2502    \u2502\n\u2502  \u2502 ViT / ResNet   \u2502  \u2502 BERT / GPT     \u2502  \u2502 Transformer    \u2502    \u2502\n\u2502  \u2502 CNN Features   \u2502  \u2502 Embeddings     \u2502  \u2502 MLP            \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502           \u2502                   \u2502                   \u2502             \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502             \u2502\n\u2502                     \u25bc                             \u2502             \u2502\n\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502             \u2502\n\u2502           \u2502  MULTIMODAL FUSION \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502           \u2502                    \u2502                                \u2502\n\u2502           \u2502 \u2022 Cross-Attention  \u2502                                \u2502\n\u2502           \u2502 \u2022 Feature Pooling  \u2502                                \u2502\n\u2502           \u2502 \u2022 Context Encoding \u2502                                \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                                           \u2502\n         \u2502                                           \u25bc\n    [Image + Text]                            [Robot Actions]\n"})}),"\n",(0,i.jsx)(e.h3,{id:"vision-encoder",children:"Vision Encoder"}),"\n",(0,i.jsx)(e.p,{children:"The vision encoder processes visual input to extract meaningful features:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom transformers import ViTModel, ViTConfig\n\nclass VisionEncoder(nn.Module):\n    """\n    Vision encoder for VLA models.\n    Can use either CNN-based (ResNet) or Transformer-based (ViT) architectures.\n    """\n\n    def __init__(self, encoder_type=\'vit\', output_dim=768):\n        super().__init__()\n        self.encoder_type = encoder_type\n        self.output_dim = output_dim\n\n        if encoder_type == \'resnet\':\n            # ResNet-based encoder\n            self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n            # Remove the final classification layer\n            self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n            self.projection = nn.Linear(2048, output_dim)\n\n        elif encoder_type == \'vit\':\n            # Vision Transformer encoder\n            config = ViTConfig(\n                image_size=224,\n                patch_size=16,\n                num_channels=3,\n                hidden_size=output_dim,\n                num_hidden_layers=12,\n                num_attention_heads=12,\n                intermediate_size=3072\n            )\n            self.backbone = ViTModel(config)\n\n        else:\n            raise ValueError(f"Unknown encoder type: {encoder_type}")\n\n    def forward(self, images):\n        """\n        Encode visual input.\n\n        Args:\n            images: Tensor of shape [batch_size, 3, height, width]\n\n        Returns:\n            Visual features of shape [batch_size, output_dim]\n        """\n        if self.encoder_type == \'resnet\':\n            features = self.backbone(images)\n            features = features.view(features.size(0), -1)  # Flatten\n            features = self.projection(features)\n\n        elif self.encoder_type == \'vit\':\n            outputs = self.backbone(images)\n            # Use CLS token representation\n            features = outputs.last_hidden_state[:, 0, :]\n\n        return features\n'})}),"\n",(0,i.jsx)(e.h3,{id:"language-encoder",children:"Language Encoder"}),"\n",(0,i.jsx)(e.p,{children:"The language encoder processes text instructions to extract semantic meaning:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n\nclass LanguageEncoder(nn.Module):\n    \"\"\"\n    Language encoder for processing text instructions.\n    Can use BERT, GPT, or other language models.\n    \"\"\"\n\n    def __init__(self, model_type='bert', output_dim=768):\n        super().__init__()\n        self.model_type = model_type\n        self.output_dim = output_dim\n\n        if model_type == 'bert':\n            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            self.model = BertModel.from_pretrained('bert-base-uncased')\n            self.projection = nn.Linear(768, output_dim)\n\n        elif model_type == 'gpt2':\n            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.model = GPT2Model.from_pretrained('gpt2')\n            self.projection = nn.Linear(768, output_dim)\n\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n    def forward(self, text_inputs):\n        \"\"\"\n        Encode text instructions.\n\n        Args:\n            text_inputs: List of text strings or tokenized inputs\n\n        Returns:\n            Language features of shape [batch_size, output_dim]\n        \"\"\"\n        if isinstance(text_inputs, list):\n            # Tokenize if raw text is provided\n            encoding = self.tokenizer(\n                text_inputs,\n                padding=True,\n                truncation=True,\n                max_length=512,\n                return_tensors='pt'\n            )\n            input_ids = encoding['input_ids'].to(self.model.device)\n            attention_mask = encoding['attention_mask'].to(self.model.device)\n        else:\n            input_ids = text_inputs['input_ids']\n            attention_mask = text_inputs['attention_mask']\n\n        if self.model_type == 'bert':\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            # Use CLS token representation\n            features = outputs.last_hidden_state[:, 0, :]\n\n        elif self.model_type == 'gpt2':\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            # Use last token representation\n            features = outputs.last_hidden_state[:, -1, :]\n\n        features = self.projection(features)\n        return features\n"})}),"\n",(0,i.jsx)(e.h3,{id:"multimodal-fusion",children:"Multimodal Fusion"}),"\n",(0,i.jsx)(e.p,{children:"The fusion module combines vision and language features:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class MultimodalFusion(nn.Module):\n    """\n    Multimodal fusion module using cross-attention.\n    Combines visual and language features for action prediction.\n    """\n\n    def __init__(self, hidden_dim=768, num_heads=8, num_layers=4):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n\n        # Cross-attention layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=0.1,\n            activation=\'gelu\',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Learnable query tokens for action prediction\n        self.action_queries = nn.Parameter(torch.randn(1, 10, hidden_dim))\n\n        # Position encodings\n        self.pos_encoding = nn.Parameter(torch.randn(1, 100, hidden_dim))\n\n    def forward(self, vision_features, language_features):\n        """\n        Fuse vision and language features.\n\n        Args:\n            vision_features: [batch_size, vision_dim]\n            language_features: [batch_size, language_dim]\n\n        Returns:\n            Fused features: [batch_size, num_queries, hidden_dim]\n        """\n        batch_size = vision_features.size(0)\n\n        # Expand features to sequence format\n        vision_seq = vision_features.unsqueeze(1)  # [batch, 1, dim]\n        language_seq = language_features.unsqueeze(1)  # [batch, 1, dim]\n\n        # Concatenate vision and language sequences\n        multimodal_seq = torch.cat([vision_seq, language_seq], dim=1)  # [batch, 2, dim]\n\n        # Add action queries\n        action_queries = self.action_queries.expand(batch_size, -1, -1)\n        multimodal_seq = torch.cat([multimodal_seq, action_queries], dim=1)  # [batch, 12, dim]\n\n        # Add positional encoding\n        seq_len = multimodal_seq.size(1)\n        multimodal_seq = multimodal_seq + self.pos_encoding[:, :seq_len, :]\n\n        # Apply transformer\n        fused_features = self.transformer(multimodal_seq)\n\n        # Extract action query outputs\n        action_features = fused_features[:, 2:, :]  # Skip vision and language tokens\n\n        return action_features\n'})}),"\n",(0,i.jsx)(e.h3,{id:"action-decoder",children:"Action Decoder"}),"\n",(0,i.jsx)(e.p,{children:"The action decoder generates robot control commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ActionDecoder(nn.Module):\n    """\n    Action decoder for generating robot control commands.\n    Outputs joint positions, velocities, or end-effector poses.\n    """\n\n    def __init__(self, input_dim=768, action_dim=12, num_action_bins=256):\n        super().__init__()\n        self.action_dim = action_dim\n        self.num_action_bins = num_action_bins\n\n        # MLP decoder for continuous actions\n        self.continuous_decoder = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, action_dim)\n        )\n\n        # Alternative: Discretized action prediction\n        self.discrete_decoder = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, action_dim * num_action_bins)\n        )\n\n        self.use_discrete = False  # Can be toggled\n\n    def forward(self, fused_features):\n        """\n        Generate action predictions.\n\n        Args:\n            fused_features: [batch_size, num_queries, input_dim]\n\n        Returns:\n            Actions: [batch_size, num_queries, action_dim] for continuous\n                    or [batch_size, num_queries, action_dim, num_bins] for discrete\n        """\n        batch_size, num_queries, _ = fused_features.shape\n\n        # Reshape for processing\n        features_flat = fused_features.view(batch_size * num_queries, -1)\n\n        if self.use_discrete:\n            # Discrete action prediction\n            logits = self.discrete_decoder(features_flat)\n            logits = logits.view(batch_size, num_queries, self.action_dim, self.num_action_bins)\n            return logits\n        else:\n            # Continuous action prediction\n            actions = self.continuous_decoder(features_flat)\n            actions = actions.view(batch_size, num_queries, self.action_dim)\n\n            # Apply tanh to bound actions to [-1, 1]\n            actions = torch.tanh(actions)\n            return actions\n'})}),"\n",(0,i.jsx)(e.h2,{id:"complete-vla-model",children:"Complete VLA Model"}),"\n",(0,i.jsx)(e.p,{children:"Putting it all together:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class VLAModel(nn.Module):\n    """\n    Complete Vision-Language-Action model for humanoid robotics.\n    """\n\n    def __init__(\n        self,\n        vision_encoder_type=\'vit\',\n        language_encoder_type=\'bert\',\n        hidden_dim=768,\n        action_dim=12,\n        num_fusion_layers=4,\n        num_action_queries=10\n    ):\n        super().__init__()\n\n        # Encoders\n        self.vision_encoder = VisionEncoder(\n            encoder_type=vision_encoder_type,\n            output_dim=hidden_dim\n        )\n        self.language_encoder = LanguageEncoder(\n            model_type=language_encoder_type,\n            output_dim=hidden_dim\n        )\n\n        # Fusion\n        self.fusion = MultimodalFusion(\n            hidden_dim=hidden_dim,\n            num_heads=8,\n            num_layers=num_fusion_layers\n        )\n\n        # Decoder\n        self.action_decoder = ActionDecoder(\n            input_dim=hidden_dim,\n            action_dim=action_dim\n        )\n\n    def forward(self, images, text_instructions):\n        """\n        Forward pass through VLA model.\n\n        Args:\n            images: [batch_size, 3, height, width]\n            text_instructions: List of text strings or tokenized inputs\n\n        Returns:\n            actions: [batch_size, num_queries, action_dim]\n        """\n        # Encode vision\n        vision_features = self.vision_encoder(images)\n\n        # Encode language\n        language_features = self.language_encoder(text_instructions)\n\n        # Fuse modalities\n        fused_features = self.fusion(vision_features, language_features)\n\n        # Decode actions\n        actions = self.action_decoder(fused_features)\n\n        return actions\n\n    def predict_action(self, image, text_instruction):\n        """\n        Predict robot action from a single image and text instruction.\n\n        Args:\n            image: Single image tensor [3, height, width]\n            text_instruction: String describing the desired action\n\n        Returns:\n            action: Predicted action [action_dim]\n        """\n        self.eval()\n        with torch.no_grad():\n            # Add batch dimension\n            image = image.unsqueeze(0)\n            text = [text_instruction]\n\n            # Forward pass\n            actions = self.forward(image, text)\n\n            # Get first action from sequence\n            action = actions[0, 0, :]\n\n        return action\n'})}),"\n",(0,i.jsx)(e.h2,{id:"training-vla-models",children:"Training VLA Models"}),"\n",(0,i.jsx)(e.h3,{id:"dataset-requirements",children:"Dataset Requirements"}),"\n",(0,i.jsx)(e.p,{children:"VLA models require diverse, multimodal datasets:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport json\n\nclass VLADataset(Dataset):\n    \"\"\"\n    Dataset for training VLA models.\n    Each sample contains: image, text instruction, and ground-truth action.\n    \"\"\"\n\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        # Load dataset annotations\n        with open(f\"{data_dir}/annotations.json\", 'r') as f:\n            self.annotations = json.load(f)\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        sample = self.annotations[idx]\n\n        # Load image\n        image_path = f\"{self.data_dir}/images/{sample['image_id']}.jpg\"\n        image = Image.open(image_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Get text instruction\n        instruction = sample['instruction']\n\n        # Get ground-truth action\n        action = torch.tensor(sample['action'], dtype=torch.float32)\n\n        return {\n            'image': image,\n            'instruction': instruction,\n            'action': action,\n            'task_id': sample.get('task_id', 0)\n        }\n"})}),"\n",(0,i.jsx)(e.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\ndef train_vla_model(\n    model,\n    train_dataset,\n    val_dataset,\n    num_epochs=100,\n    batch_size=32,\n    learning_rate=1e-4\n):\n    \"\"\"Train VLA model on dataset.\"\"\"\n\n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=4\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4\n    )\n\n    # Optimizer and loss\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    criterion = nn.MSELoss()  # For continuous actions\n\n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=num_epochs\n    )\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n\n        for batch in train_loader:\n            images = batch['image'].to(device)\n            instructions = batch['instruction']\n            actions_gt = batch['action'].to(device)\n\n            # Forward pass\n            actions_pred = model(images, instructions)\n\n            # Take first action from sequence\n            actions_pred = actions_pred[:, 0, :]\n\n            # Compute loss\n            loss = criterion(actions_pred, actions_gt)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch['image'].to(device)\n                instructions = batch['instruction']\n                actions_gt = batch['action'].to(device)\n\n                actions_pred = model(images, instructions)\n                actions_pred = actions_pred[:, 0, :]\n\n                loss = criterion(actions_pred, actions_gt)\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_vla_model.pth')\n\n        scheduler.step()\n\n    return model\n"})}),"\n",(0,i.jsx)(e.h2,{id:"vla-model-variants",children:"VLA Model Variants"}),"\n",(0,i.jsx)(e.h3,{id:"rt-1-style-architecture",children:"RT-1 Style Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RT1Model(VLAModel):\n    """\n    Robotics Transformer 1 (RT-1) style architecture.\n    Uses tokenized actions and autoregressive prediction.\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # RT-1 uses discrete action tokens\n        self.action_decoder.use_discrete = True\n\n        # Add autoregressive decoder\n        self.autoregressive_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(\n                d_model=kwargs.get(\'hidden_dim\', 768),\n                nhead=8,\n                dim_feedforward=2048\n            ),\n            num_layers=4\n        )\n'})}),"\n",(0,i.jsx)(e.h3,{id:"rt-2-with-vision-language-pre-training",children:"RT-2 with Vision-Language Pre-training"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RT2Model(VLAModel):\n    """\n    RT-2 style model with vision-language pre-training.\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Use pre-trained vision-language model (e.g., CLIP)\n        from transformers import CLIPModel\n        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n\n        # Freeze CLIP parameters initially\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n'})}),"\n",(0,i.jsx)(e.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def optimize_vla_for_deployment(model, example_inputs):\n    \"\"\"Optimize VLA model for deployment on robot hardware.\"\"\"\n\n    # Quantization\n    model_int8 = torch.quantization.quantize_dynamic(\n        model, {nn.Linear}, dtype=torch.qint8\n    )\n\n    # ONNX export\n    torch.onnx.export(\n        model,\n        example_inputs,\n        \"vla_model.onnx\",\n        input_names=['image', 'text'],\n        output_names=['action'],\n        dynamic_axes={\n            'image': {0: 'batch_size'},\n            'text': {0: 'batch_size'},\n            'action': {0: 'batch_size'}\n        }\n    )\n\n    return model_int8\n"})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Data Diversity"}),": Train on diverse tasks and environments"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Task Learning"}),": Share representations across tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pre-training"}),": Leverage vision-language pre-training (CLIP, ALIGN)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Regularization"}),": Use dropout, weight decay, and data augmentation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Evaluation"}),": Test on held-out tasks and environments"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety"}),": Implement safety checks and human oversight"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Iterative Improvement"}),": Continuously collect data and fine-tune"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The VLA paradigm enables humanoid robots to understand and execute complex tasks through natural language instructions, bridging the gap between human intent and robot action."})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(c,{...n})}):c(n)}},8453(n,e,t){t.d(e,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);