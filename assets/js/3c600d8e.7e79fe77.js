"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[4678],{2774(n,i,e){e.r(i),e.d(i,{assets:()=>u,contentTitle:()=>c,default:()=>p,frontMatter:()=>r,metadata:()=>d,toc:()=>m});var t=e(4848),o=e(8453),a=e(4252),s=e(1470),l=e(9365);const r={title:"Unity Visualization",description:"Learn about Unity for advanced visualization and interaction in humanoid robotics",tags:["unity","visualization","simulation","robotics","graphics"],sidebar_label:"Unity Visualization",sidebar_position:4,keywords:["unity","visualization","robotics","graphics","simulation"],toc_min_heading_level:2,toc_max_heading_level:4},c="Unity Visualization",d={id:"module-2-digital-twin/unity-visualization",title:"Unity Visualization",description:"Learn about Unity for advanced visualization and interaction in humanoid robotics",source:"@site/docs/module-2-digital-twin/unity-visualization.mdx",sourceDirName:"module-2-digital-twin",slug:"/module-2-digital-twin/unity-visualization",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/unity-visualization",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-2-digital-twin/unity-visualization.mdx",tags:[{label:"unity",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/tags/unity"},{label:"visualization",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/tags/visualization"},{label:"simulation",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/tags/simulation"},{label:"robotics",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/tags/robotics"},{label:"graphics",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/tags/graphics"}],version:"current",sidebarPosition:4,frontMatter:{title:"Unity Visualization",description:"Learn about Unity for advanced visualization and interaction in humanoid robotics",tags:["unity","visualization","simulation","robotics","graphics"],sidebar_label:"Unity Visualization",sidebar_position:4,keywords:["unity","visualization","robotics","graphics","simulation"],toc_min_heading_level:2,toc_max_heading_level:4},sidebar:"tutorialSidebar",previous:{title:"Gazebo Physics",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/gazebo-physics"},next:{title:"ROS 2 Integration",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/ros2-integration"}},u={},m=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deep Explanation",id:"deep-explanation",level:2},{value:"Unity in Robotics Context",id:"unity-in-robotics-context",level:3},{value:"Visualization Platform",id:"visualization-platform",level:4},{value:"Interaction Interface",id:"interaction-interface",level:4},{value:"Simulation Environment",id:"simulation-environment",level:4},{value:"Unity Robotics Ecosystem",id:"unity-robotics-ecosystem",level:3},{value:"Unity Robotics Hub",id:"unity-robotics-hub",level:4},{value:"Unity ML-Agents",id:"unity-ml-agents",level:4},{value:"Unity Perception",id:"unity-perception",level:4},{value:"Visualization Techniques for Robotics",id:"visualization-techniques-for-robotics",level:3},{value:"Robot State Visualization",id:"robot-state-visualization",level:4},{value:"Environmental Visualization",id:"environmental-visualization",level:4},{value:"Data Visualization",id:"data-visualization",level:4},{value:"Unity-ROS Integration",id:"unity-ros-integration",level:3},{value:"ROS# Communication Library",id:"ros-communication-library",level:4},{value:"Message Handling",id:"message-handling",level:4},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Real-time Requirements",id:"real-time-requirements",level:4},{value:"Resource Management",id:"resource-management",level:4},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Unity Robot Visualization Setup",id:"example-1-unity-robot-visualization-setup",level:3},{value:"Example 2: Unity Environment and Human-Robot Interaction",id:"example-2-unity-environment-and-human-robot-interaction",level:3},{value:"Exercises and Checkpoints",id:"exercises-and-checkpoints",level:2},{value:"Exercise 1: Unity Environment Design",id:"exercise-1-unity-environment-design",level:3},{value:"Exercise 2: ROS-Unity Integration",id:"exercise-2-ros-unity-integration",level:3},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:3},{value:"Summary and Key Takeaways",id:"summary-and-key-takeaways",level:2},{value:"Key Concepts Recap",id:"key-concepts-recap",level:3},{value:"Practical Applications",id:"practical-applications",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"Common Mistakes and Troubleshooting",id:"common-mistakes-and-troubleshooting",level:3},{value:"References and Resources",id:"references-and-resources",level:3}];function h(n){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"unity-visualization",children:"Unity Visualization"}),"\n","\n","\n",(0,t.jsx)(a.A,{toc:m}),"\n",(0,t.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(i.p,{children:"Unity has emerged as a powerful platform for robotics visualization and simulation, offering high-quality graphics, advanced rendering capabilities, and sophisticated interaction systems. In this chapter, we'll explore how Unity can be leveraged for humanoid robotics applications, from creating photorealistic simulation environments to developing intuitive human-robot interaction interfaces."}),"\n",(0,t.jsx)(i.p,{children:"Unity's strength lies in its ability to create visually compelling and interactive experiences that can enhance robot development, testing, and human-robot interaction. Combined with its robust physics engine and extensive asset ecosystem, Unity provides a unique environment for creating advanced digital twins that bridge the gap between simulation and reality."}),"\n",(0,t.jsx)(i.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Understand Unity's role in robotics visualization and simulation"}),"\n",(0,t.jsx)(i.li,{children:"Create realistic 3D environments for humanoid robot simulation"}),"\n",(0,t.jsx)(i.li,{children:"Implement advanced visualization techniques for robot data"}),"\n",(0,t.jsx)(i.li,{children:"Develop human-robot interaction interfaces using Unity"}),"\n",(0,t.jsx)(i.li,{children:"Integrate Unity with ROS 2 systems for bidirectional communication"}),"\n",(0,t.jsx)(i.li,{children:"Optimize Unity scenes for real-time robotics applications"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Basic understanding of Unity development environment"}),"\n",(0,t.jsx)(i.li,{children:"Knowledge of 3D graphics concepts"}),"\n",(0,t.jsx)(i.li,{children:"Understanding of ROS 2 communication patterns from Module 1"}),"\n",(0,t.jsx)(i.li,{children:"Familiarity with humanoid robot kinematics and dynamics"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"deep-explanation",children:"Deep Explanation"}),"\n",(0,t.jsx)(i.h3,{id:"unity-in-robotics-context",children:"Unity in Robotics Context"}),"\n",(0,t.jsx)(i.p,{children:"Unity serves several important roles in robotics development:"}),"\n",(0,t.jsx)(i.h4,{id:"visualization-platform",children:"Visualization Platform"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"High-quality rendering for robot and environment visualization"}),"\n",(0,t.jsx)(i.li,{children:"Real-time camera feeds and sensor data visualization"}),"\n",(0,t.jsx)(i.li,{children:"Advanced lighting and material systems"}),"\n",(0,t.jsx)(i.li,{children:"Photorealistic rendering capabilities"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"interaction-interface",children:"Interaction Interface"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Intuitive user interfaces for robot control and monitoring"}),"\n",(0,t.jsx)(i.li,{children:"Virtual reality (VR) and augmented reality (AR) support"}),"\n",(0,t.jsx)(i.li,{children:"Multi-modal interaction systems"}),"\n",(0,t.jsx)(i.li,{children:"Remote operation interfaces"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"simulation-environment",children:"Simulation Environment"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Physics simulation capabilities (though typically combined with other engines for robotics)"}),"\n",(0,t.jsx)(i.li,{children:"Environmental modeling and scenario creation"}),"\n",(0,t.jsx)(i.li,{children:"Human-in-the-loop simulation"}),"\n",(0,t.jsx)(i.li,{children:"Training environment for AI development"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"unity-robotics-ecosystem",children:"Unity Robotics Ecosystem"}),"\n",(0,t.jsx)(i.p,{children:"Unity provides several tools and packages specifically for robotics:"}),"\n",(0,t.jsx)(i.h4,{id:"unity-robotics-hub",children:"Unity Robotics Hub"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Collection of tools, samples, and documentation"}),"\n",(0,t.jsx)(i.li,{children:"ROS# communication library"}),"\n",(0,t.jsx)(i.li,{children:"Robot samples and tutorials"}),"\n",(0,t.jsx)(i.li,{children:"Best practices and guidelines"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"unity-ml-agents",children:"Unity ML-Agents"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Reinforcement learning toolkit"}),"\n",(0,t.jsx)(i.li,{children:"Training AI agents in Unity environments"}),"\n",(0,t.jsx)(i.li,{children:"Integration with popular ML frameworks"}),"\n",(0,t.jsx)(i.li,{children:"Curriculum learning capabilities"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"unity-perception",children:"Unity Perception"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Synthetic data generation for computer vision"}),"\n",(0,t.jsx)(i.li,{children:"Domain randomization techniques"}),"\n",(0,t.jsx)(i.li,{children:"Sensor simulation tools"}),"\n",(0,t.jsx)(i.li,{children:"Annotation and dataset generation"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"visualization-techniques-for-robotics",children:"Visualization Techniques for Robotics"}),"\n",(0,t.jsx)(i.p,{children:"Effective robotics visualization requires specialized techniques:"}),"\n",(0,t.jsx)(i.h4,{id:"robot-state-visualization",children:"Robot State Visualization"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Joint position and velocity displays"}),"\n",(0,t.jsx)(i.li,{children:"Trajectory and path visualization"}),"\n",(0,t.jsx)(i.li,{children:"Force and torque visualization"}),"\n",(0,t.jsx)(i.li,{children:"Sensor data overlay"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"environmental-visualization",children:"Environmental Visualization"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"3D mapping and localization visualization"}),"\n",(0,t.jsx)(i.li,{children:"Occupancy grids and point clouds"}),"\n",(0,t.jsx)(i.li,{children:"Dynamic obstacle representation"}),"\n",(0,t.jsx)(i.li,{children:"Multi-sensor fusion visualization"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"data-visualization",children:"Data Visualization"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Real-time plotting of robot metrics"}),"\n",(0,t.jsx)(i.li,{children:"Performance analytics dashboards"}),"\n",(0,t.jsx)(i.li,{children:"Sensor fusion visualization"}),"\n",(0,t.jsx)(i.li,{children:"Behavior and decision visualization"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"unity-ros-integration",children:"Unity-ROS Integration"}),"\n",(0,t.jsx)(i.p,{children:"The integration between Unity and ROS 2 enables bidirectional communication:"}),"\n",(0,t.jsx)(i.h4,{id:"ros-communication-library",children:"ROS# Communication Library"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:".NET-based ROS client for Unity"}),"\n",(0,t.jsx)(i.li,{children:"Support for common ROS message types"}),"\n",(0,t.jsx)(i.li,{children:"Service and action client implementations"}),"\n",(0,t.jsx)(i.li,{children:"Real-time communication capabilities"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"message-handling",children:"Message Handling"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Custom message type generation"}),"\n",(0,t.jsx)(i.li,{children:"Efficient serialization/deserialization"}),"\n",(0,t.jsx)(i.li,{children:"Quality of Service (QoS) configuration"}),"\n",(0,t.jsx)(i.li,{children:"Connection management and error handling"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,t.jsx)(i.p,{children:"Unity applications for robotics must balance visual quality with performance:"}),"\n",(0,t.jsx)(i.h4,{id:"real-time-requirements",children:"Real-time Requirements"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Maintaining high frame rates for interaction"}),"\n",(0,t.jsx)(i.li,{children:"Efficient rendering techniques"}),"\n",(0,t.jsx)(i.li,{children:"Level of detail (LOD) systems"}),"\n",(0,t.jsx)(i.li,{children:"Occlusion culling and frustum culling"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"resource-management",children:"Resource Management"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Memory optimization for long-running applications"}),"\n",(0,t.jsx)(i.li,{children:"Asset streaming and loading strategies"}),"\n",(0,t.jsx)(i.li,{children:"Physics simulation optimization"}),"\n",(0,t.jsx)(i.li,{children:"Network communication efficiency"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,t.jsx)(i.h3,{id:"example-1-unity-robot-visualization-setup",children:"Example 1: Unity Robot Visualization Setup"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsx)(l.A,{value:"robot_controller",label:"Robot Controller Script",default:!0,children:(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Geometry;\n\npublic class RobotController : MonoBehaviour\n{\n    [Header("ROS Connection")]\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n\n    [Header("Robot Configuration")]\n    public string jointStatesTopic = "/joint_states";\n    public string cmdVelTopic = "/cmd_vel";\n\n    [Header("Joint Mapping")]\n    public Dictionary<string, Transform> jointMap = new Dictionary<string, Transform>();\n\n    [Header("Visualization Settings")]\n    public float positionScale = 1.0f;\n    public float rotationScale = 1.0f;\n\n    private ROSConnection ros;\n    private JointStateMsg latestJointState;\n\n    void Start()\n    {\n        // Initialize ROS connection\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(rosIPAddress, rosPort);\n\n        // Subscribe to joint states\n        ros.Subscribe<JointStateMsg>(jointStatesTopic, OnJointStateReceived);\n\n        // Initialize joint mapping (this would typically be set up in the Unity Editor)\n        InitializeJointMap();\n    }\n\n    void InitializeJointMap()\n    {\n        // This is where you would map joint names to Unity transforms\n        // In a real implementation, this would be done via the Unity Editor\n        // or by searching for child objects with specific naming conventions\n\n        // Example mapping (you would replace these with actual joint names and transforms):\n        Transform[] allChildren = GetComponentsInChildren<Transform>();\n        foreach (Transform child in allChildren)\n        {\n            if (child.name.Contains("joint") || child.name.Contains("Joint"))\n            {\n                jointMap[child.name.ToLower()] = child;\n            }\n        }\n    }\n\n    void OnJointStateReceived(JointStateMsg msg)\n    {\n        latestJointState = msg;\n        UpdateRobotVisualization();\n    }\n\n    void UpdateRobotVisualization()\n    {\n        if (latestJointState == null || latestJointState.name == null)\n            return;\n\n        // Update each joint based on received positions\n        for (int i = 0; i < latestJointState.name.Length; i++)\n        {\n            string jointName = latestJointState.name[i].ToLower();\n            float jointPosition = latestJointState.position[i];\n\n            if (jointMap.ContainsKey(jointName))\n            {\n                Transform jointTransform = jointMap[jointName];\n\n                // Apply rotation based on joint position\n                // This assumes the joint rotates around the Z-axis\n                // You may need to adjust the rotation axis based on your robot model\n                jointTransform.localRotation = Quaternion.Euler(0, 0, jointPosition * Mathf.Rad2Deg * rotationScale);\n            }\n        }\n    }\n\n    void Update()\n    {\n        // Continuous update loop for real-time visualization\n        // Additional visualization updates can be performed here\n    }\n\n    // Method to send velocity commands to the robot\n    public void SendVelocityCommand(float linearX, float angularZ)\n    {\n        var cmdVel = new TwistMsg();\n        cmdVel.linear = new Vector3Msg(linearX, 0, 0);  // Move forward/backward\n        cmdVel.angular = new Vector3Msg(0, 0, angularZ);  // Rotate left/right\n\n        ros.Publish(cmdVelTopic, cmdVel);\n    }\n}\n'})})}),(0,t.jsx)(l.A,{value:"sensor_visualizer",label:"Sensor Data Visualizer",children:(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Nav;\n\npublic class SensorDataVisualizer : MonoBehaviour\n{\n    [Header("Topic Configuration")]\n    public string laserScanTopic = "/scan";\n    public string imuTopic = "/imu/data";\n    public string cameraTopic = "/camera/image_raw";  // This would be handled differently in practice\n\n    [Header("Visualization Settings")]\n    public GameObject laserScanPrefab;\n    public Material laserMaterial;\n    public float maxLaserDistance = 10.0f;\n    public int laserScanResolution = 360;\n\n    [Header("IMU Visualization")]\n    public GameObject imuVisualizer;\n    public float imuScale = 1.0f;\n\n    private ROSConnection ros;\n    private LaserScanMsg latestLaserScan;\n    private ImuMsg latestImuData;\n    private LineRenderer laserLineRenderer;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Subscribe to sensor topics\n        ros.Subscribe<LaserScanMsg>(laserScanTopic, OnLaserScanReceived);\n        ros.Subscribe<ImuMsg>(imuTopic, OnImuReceived);\n\n        // Setup laser scan visualization\n        SetupLaserScanVisualization();\n    }\n\n    void SetupLaserScanVisualization()\n    {\n        if (laserScanPrefab != null)\n        {\n            GameObject laserScanGO = Instantiate(laserScanPrefab, transform);\n            laserLineRenderer = laserScanGO.GetComponent<LineRenderer>();\n\n            if (laserLineRenderer != null)\n            {\n                laserLineRenderer.material = laserMaterial;\n                laserLineRenderer.positionCount = laserScanResolution;\n                laserLineRenderer.useWorldSpace = false;\n            }\n        }\n    }\n\n    void OnLaserScanReceived(LaserScanMsg msg)\n    {\n        latestLaserScan = msg;\n        UpdateLaserScanVisualization();\n    }\n\n    void OnImuReceived(ImuMsg msg)\n    {\n        latestImuData = msg;\n        UpdateImuVisualization();\n    }\n\n    void UpdateLaserScanVisualization()\n    {\n        if (latestLaserScan == null || laserLineRenderer == null)\n            return;\n\n        // Update line renderer with laser scan data\n        Vector3[] points = new Vector3[latestLaserScan.ranges.Length];\n\n        for (int i = 0; i < latestLaserScan.ranges.Length; i++)\n        {\n            float angle = latestLaserScan.angle_min + (i * latestLaserScan.angle_increment);\n            float distance = latestLaserScan.ranges[i];\n\n            // Limit distance to max for visualization\n            if (float.IsNaN(distance) || float.IsInfinity(distance))\n                distance = maxLaserDistance;\n            else\n                distance = Mathf.Min(distance, maxLaserDistance);\n\n            // Calculate position in 2D polar coordinates\n            float x = distance * Mathf.Cos(angle);\n            float y = distance * Mathf.Sin(angle);\n\n            points[i] = new Vector3(x, 0, y);  // Unity Y is up, so laser is in XZ plane\n        }\n\n        laserLineRenderer.positionCount = points.Length;\n        laserLineRenderer.SetPositions(points);\n    }\n\n    void UpdateImuVisualization()\n    {\n        if (latestImuData == null || imuVisualizer == null)\n            return;\n\n        // Convert quaternion from IMU to Unity rotation\n        // ROS uses a different coordinate system than Unity\n        var orientation = latestImuData.orientation;\n\n        // Convert ROS quaternion (x, y, z, w) to Unity quaternion\n        // ROS: X forward, Y left, Z up\n        // Unity: X right, Y up, Z forward\n        Quaternion unityRotation = new Quaternion(\n            orientation.y,    // ROS Y -> Unity X\n            orientation.z,    // ROS Z -> Unity Y\n            orientation.x,    // ROS X -> Unity Z\n            orientation.w     // W remains W\n        );\n\n        // Apply the rotation to the IMU visualizer\n        imuVisualizer.transform.rotation = unityRotation * Quaternion.Euler(90, 0, 0);\n    }\n\n    // Additional visualization methods\n    public void HighlightObstacle(float x, float y, float z)\n    {\n        // Create a visual indicator for detected obstacles\n        GameObject obstacleIndicator = GameObject.CreatePrimitive(PrimitiveType.Sphere);\n        obstacleIndicator.transform.position = new Vector3(x, y, z);\n        obstacleIndicator.transform.localScale = Vector3.one * 0.1f;\n\n        Renderer renderer = obstacleIndicator.GetComponent<Renderer>();\n        if (renderer != null)\n        {\n            renderer.material = new Material(Shader.Find("Unlit/Color"));\n            renderer.material.color = Color.red;\n        }\n\n        // Destroy after a few seconds\n        Destroy(obstacleIndicator, 3.0f);\n    }\n}\n'})})})]}),"\n",(0,t.jsx)(i.h3,{id:"example-2-unity-environment-and-human-robot-interaction",children:"Example 2: Unity Environment and Human-Robot Interaction"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsx)(l.A,{value:"environment_manager",label:"Environment Manager",children:(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Geometry;\n\npublic class EnvironmentManager : MonoBehaviour\n{\n    [Header("Environment Configuration")]\n    public GameObject[] interactiveObjects;\n    public Material[] environmentMaterials;\n    public Light[] sceneLights;\n\n    [Header("Human-Robot Interaction")]\n    public Camera mainCamera;\n    public float interactionDistance = 5.0f;\n    public LayerMask interactionLayer;\n\n    [Header("ROS Communication")]\n    public string objectDetectionTopic = "/detected_objects";\n    public string navigationGoalTopic = "/move_base_simple/goal";\n\n    private ROSConnection ros;\n    private List<GameObject> spawnedObjects = new List<GameObject>();\n    private RaycastHit interactionHit;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Initialize environment\n        SetupEnvironment();\n    }\n\n    void SetupEnvironment()\n    {\n        // Apply random materials to environment objects for variety\n        foreach (GameObject obj in interactiveObjects)\n        {\n            if (obj != null && obj.GetComponent<Renderer>() != null)\n            {\n                Material randomMaterial = environmentMaterials[Random.Range(0, environmentMaterials.Length)];\n                obj.GetComponent<Renderer>().material = randomMaterial;\n            }\n        }\n\n        // Configure lights\n        foreach (Light light in sceneLights)\n        {\n            if (light != null)\n            {\n                light.intensity = Random.Range(0.8f, 1.2f);\n                light.color = GetRandomWarmColor();\n            }\n        }\n    }\n\n    Color GetRandomWarmColor()\n    {\n        // Generate warm light colors (yellows, oranges, whites)\n        float r = Random.Range(0.8f, 1.0f);\n        float g = Random.Range(0.7f, 1.0f);\n        float b = Random.Range(0.6f, 0.9f);\n        return new Color(r, g, b);\n    }\n\n    void Update()\n    {\n        HandleUserInput();\n        HandleRaycastInteraction();\n    }\n\n    void HandleUserInput()\n    {\n        // Handle mouse clicks for interaction\n        if (Input.GetMouseButtonDown(0))\n        {\n            Ray ray = mainCamera.ScreenPointToRay(Input.mousePosition);\n            RaycastHit hit;\n\n            if (Physics.Raycast(ray, out hit, interactionDistance, interactionLayer))\n            {\n                // Object clicked - send interaction command to robot\n                HandleObjectInteraction(hit.collider.gameObject, hit.point);\n            }\n        }\n\n        // Handle keyboard commands\n        if (Input.GetKeyDown(KeyCode.Space))\n        {\n            SendRobotStopCommand();\n        }\n\n        if (Input.GetKeyDown(KeyCode.R))\n        {\n            ResetEnvironment();\n        }\n    }\n\n    void HandleRaycastInteraction()\n    {\n        // Continuous raycast for highlighting\n        Ray ray = mainCamera.ScreenPointToRay(Input.mousePosition);\n\n        if (Physics.Raycast(ray, out interactionHit, interactionDistance, interactionLayer))\n        {\n            // Highlight the object being looked at\n            HighlightObject(interactionHit.collider.gameObject, true);\n        }\n        else\n        {\n            // Remove highlight from previously highlighted object\n            if (interactionHit.collider != null)\n            {\n                HighlightObject(interactionHit.collider.gameObject, false);\n                interactionHit = new RaycastHit(); // Reset\n            }\n        }\n    }\n\n    void HandleObjectInteraction(GameObject targetObject, Vector3 hitPoint)\n    {\n        // Determine what type of interaction to send to the robot\n        string objectType = targetObject.tag;  // Assuming objects have tags\n\n        switch (objectType)\n        {\n            case "NavigablePoint":\n                SendNavigationGoal(hitPoint);\n                break;\n            case "InteractiveObject":\n                SendManipulationCommand(targetObject.name, hitPoint);\n                break;\n            case "Obstacle":\n                SendAvoidanceCommand(hitPoint);\n                break;\n            default:\n                Debug.Log("Unknown object type: " + objectType);\n                break;\n        }\n    }\n\n    void SendNavigationGoal(Vector3 goalPosition)\n    {\n        // Send a navigation goal to the robot\n        var goal = new PoseStampedMsg();\n        goal.header = new HeaderMsg();\n        goal.header.stamp = new TimeMsg(System.DateTime.UtcNow.Second, System.DateTime.UtcNow.Millisecond * 1000000);\n        goal.header.frame_id = "map";\n\n        // Convert Unity coordinates to ROS coordinates\n        goal.pose = new PoseMsg();\n        goal.pose.position = new Vector3Msg(goalPosition.x, goalPosition.z, goalPosition.y);  // Unity Y->ROS Z, Unity Z->ROS Y\n        goal.pose.orientation = new QuaternionMsg(0, 0, 0, 1);  // Default orientation\n\n        // In a real implementation, you would publish this to the navigation system\n        Debug.Log($"Sending navigation goal to: ({goal.pose.position.x}, {goal.pose.position.y}, {goal.pose.position.z})");\n    }\n\n    void SendManipulationCommand(string objectName, Vector3 targetPosition)\n    {\n        // Send manipulation command to the robot\n        Debug.Log($"Sending manipulation command for object: {objectName} at ({targetPosition.x}, {targetPosition.y}, {targetPosition.z})");\n\n        // In a real implementation, you would send specific manipulation commands via ROS\n        // This could include pick/place commands, grasp poses, etc.\n    }\n\n    void SendAvoidanceCommand(Vector3 obstaclePosition)\n    {\n        // Send obstacle avoidance command\n        Debug.Log($"Sending obstacle avoidance for: ({obstaclePosition.x}, {obstaclePosition.y}, {obstaclePosition.z})");\n    }\n\n    void SendRobotStopCommand()\n    {\n        // Send emergency stop command to robot\n        Debug.Log("Sending robot stop command");\n\n        // In a real implementation, you would publish a zero velocity command\n        // or call an emergency stop service\n    }\n\n    void HighlightObject(GameObject obj, bool highlight)\n    {\n        if (obj == null) return;\n\n        Renderer renderer = obj.GetComponent<Renderer>();\n        if (renderer != null)\n        {\n            if (highlight)\n            {\n                // Store original material to restore later\n                if (!obj.GetComponent<ObjectHighlight>())\n                {\n                    ObjectHighlight highlightComponent = obj.AddComponent<ObjectHighlight>();\n                    highlightComponent.originalMaterial = renderer.material;\n                }\n\n                // Apply highlight material\n                renderer.material = CreateHighlightMaterial();\n            }\n            else\n            {\n                // Restore original material\n                ObjectHighlight highlightComponent = obj.GetComponent<ObjectHighlight>();\n                if (highlightComponent != null && highlightComponent.originalMaterial != null)\n                {\n                    renderer.material = highlightComponent.originalMaterial;\n                }\n            }\n        }\n    }\n\n    Material CreateHighlightMaterial()\n    {\n        Material highlightMat = new Material(Shader.Find("Standard"));\n        highlightMat.color = Color.yellow;\n        highlightMat.SetFloat("_Metallic", 0.5f);\n        highlightMat.SetFloat("_Smoothness", 0.5f);\n        return highlightMat;\n    }\n\n    void ResetEnvironment()\n    {\n        // Reset the environment to initial state\n        Debug.Log("Resetting environment");\n\n        // Clear spawned objects\n        foreach (GameObject obj in spawnedObjects)\n        {\n            if (obj != null)\n                Destroy(obj);\n        }\n        spawnedObjects.Clear();\n\n        // Reset environment objects to initial state\n        SetupEnvironment();\n    }\n}\n\n// Helper class to store original materials\npublic class ObjectHighlight : MonoBehaviour\n{\n    public Material originalMaterial;\n}\n'})})}),(0,t.jsx)(l.A,{value:"ui_manager",label:"UI Manager",children:(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.UI;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Std;\n\npublic class UIManager : MonoBehaviour\n{\n    [Header("UI Elements")]\n    public Text robotStatusText;\n    public Text jointStateText;\n    public Text sensorDataText;\n    public Slider simulationSpeedSlider;\n    public Button resetSimulationButton;\n    public Toggle pauseSimulationToggle;\n\n    [Header("Visualization Panels")]\n    public GameObject jointControlPanel;\n    public GameObject sensorVisualizationPanel;\n    public GameObject robotControlPanel;\n\n    [Header("ROS Configuration")]\n    public string robotStatusTopic = "/robot_status";\n    public string jointStatesTopic = "/joint_states";\n\n    private ROSConnection ros;\n    private JointStateMsg latestJointState;\n    private bool simulationPaused = false;\n    private float simulationSpeed = 1.0f;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Subscribe to robot status\n        ros.Subscribe<StringMsg>(robotStatusTopic, OnRobotStatusReceived);\n        ros.Subscribe<JointStateMsg>(jointStatesTopic, OnJointStateReceived);\n\n        // Setup UI event handlers\n        SetupUIEvents();\n\n        // Initialize UI\n        UpdateRobotStatusDisplay();\n        UpdateJointStateDisplay();\n    }\n\n    void SetupUIEvents()\n    {\n        if (simulationSpeedSlider != null)\n        {\n            simulationSpeedSlider.onValueChanged.AddListener(OnSimulationSpeedChanged);\n        }\n\n        if (resetSimulationButton != null)\n        {\n            resetSimulationButton.onClick.AddListener(OnResetSimulationClicked);\n        }\n\n        if (pauseSimulationToggle != null)\n        {\n            pauseSimulationToggle.onValueChanged.AddListener(OnPauseToggled);\n        }\n    }\n\n    void OnRobotStatusReceived(StringMsg msg)\n    {\n        // Update robot status from ROS message\n        if (robotStatusText != null)\n        {\n            robotStatusText.text = "Robot Status: " + msg.data;\n        }\n    }\n\n    void OnJointStateReceived(JointStateMsg msg)\n    {\n        latestJointState = msg;\n        UpdateJointStateDisplay();\n    }\n\n    void UpdateRobotStatusDisplay()\n    {\n        if (robotStatusText != null)\n        {\n            robotStatusText.text = "Robot Status: " + (simulationPaused ? "PAUSED" : "RUNNING");\n        }\n    }\n\n    void UpdateJointStateDisplay()\n    {\n        if (latestJointState != null && jointStateText != null)\n        {\n            string jointInfo = "Joint States:\\n";\n\n            for (int i = 0; i < Mathf.Min(latestJointState.name.Length, latestJointState.position.Length); i++)\n            {\n                jointInfo += $"{latestJointState.name[i]}: {latestJointState.position[i]:F3} rad";\n\n                if (i < latestJointState.velocity.Length)\n                    jointInfo += $" ({latestJointState.velocity[i]:F3} rad/s)";\n\n                jointInfo += "\\n";\n            }\n\n            jointStateText.text = jointInfo;\n        }\n    }\n\n    void UpdateSensorDataDisplay()\n    {\n        if (sensorDataText != null)\n        {\n            // This would be updated with real sensor data\n            sensorDataText.text = "Sensor Data:\\n" +\n                                 "IMU: Connected\\n" +\n                                 "Cameras: 2/2 Online\\n" +\n                                 "LIDAR: Connected\\n" +\n                                 "Force/Torque: 6/6 Active";\n        }\n    }\n\n    public void OnSimulationSpeedChanged(float value)\n    {\n        simulationSpeed = value;\n        Debug.Log($"Simulation speed changed to: {simulationSpeed}x");\n\n        // In a real implementation, you would send this to the simulation engine\n        // This might involve adjusting physics time steps or animation speeds\n    }\n\n    public void OnResetSimulationClicked()\n    {\n        Debug.Log("Reset simulation clicked");\n\n        // Reset all simulation components\n        ResetRobotPosition();\n        ResetEnvironment();\n        ClearVisualization();\n    }\n\n    public void OnPauseToggled(bool paused)\n    {\n        simulationPaused = paused;\n        UpdateRobotStatusDisplay();\n\n        // In a real implementation, you would pause/unpause the simulation\n        // This might involve stopping physics updates, animations, etc.\n    }\n\n    void ResetRobotPosition()\n    {\n        // Reset robot to initial position\n        // This would involve sending commands to reset the robot in the simulation\n        Debug.Log("Resetting robot position");\n    }\n\n    void ResetEnvironment()\n    {\n        // Reset environment objects to initial state\n        Debug.Log("Resetting environment");\n    }\n\n    void ClearVisualization()\n    {\n        // Clear any temporary visualization elements\n        Debug.Log("Clearing visualization");\n    }\n\n    void Update()\n    {\n        // Update UI elements that need continuous updates\n        UpdateSensorDataDisplay();\n\n        // Update simulation if not paused\n        if (!simulationPaused)\n        {\n            // Simulation updates would happen here\n        }\n    }\n\n    // Methods for controlling robot from UI\n    public void SendVelocityCommand(float linear, float angular)\n    {\n        // Send velocity command to robot\n        Debug.Log($"Sending velocity command: linear={linear}, angular={angular}");\n\n        // In a real implementation, you would publish this to /cmd_vel\n    }\n\n    public void SendJointPositionCommand(string jointName, float position)\n    {\n        // Send joint position command\n        Debug.Log($"Sending joint command: {jointName} = {position}");\n\n        // In a real implementation, you would publish this to the appropriate joint controller\n    }\n\n    // Toggle panel visibility\n    public void ToggleJointControlPanel()\n    {\n        if (jointControlPanel != null)\n            jointControlPanel.SetActive(!jointControlPanel.activeSelf);\n    }\n\n    public void ToggleSensorVisualizationPanel()\n    {\n        if (sensorVisualizationPanel != null)\n            sensorVisualizationPanel.SetActive(!sensorVisualizationPanel.activeSelf);\n    }\n\n    public void ToggleRobotControlPanel()\n    {\n        if (robotControlPanel != null)\n            robotControlPanel.SetActive(!robotControlPanel.activeSelf);\n    }\n}\n'})})})]}),"\n",(0,t.jsx)(i.h2,{id:"exercises-and-checkpoints",children:"Exercises and Checkpoints"}),"\n",(0,t.jsx)(i.h3,{id:"exercise-1-unity-environment-design",children:"Exercise 1: Unity Environment Design"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Scenario:"})," You need to create a Unity environment for testing a humanoid robot's navigation and manipulation capabilities. The environment should include:"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"A realistic indoor setting with furniture and obstacles"}),"\n",(0,t.jsx)(i.li,{children:"Interactive objects that the robot can manipulate"}),"\n",(0,t.jsx)(i.li,{children:"Sensor visualization capabilities"}),"\n",(0,t.jsx)(i.li,{children:"Human-robot interaction interfaces"}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Task:"})," Design and implement the Unity scene with:"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Appropriate lighting and materials for realistic rendering"}),"\n",(0,t.jsx)(i.li,{children:"Physics colliders for proper interaction"}),"\n",(0,t.jsx)(i.li,{children:"Sensor visualization systems"}),"\n",(0,t.jsx)(i.li,{children:"User interface for robot control and monitoring"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Success Criteria:"})}),"\n",(0,t.jsxs)(i.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Realistic environment with appropriate scale"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Functional physics interactions"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Clear sensor data visualization"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Intuitive user interface"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"exercise-2-ros-unity-integration",children:"Exercise 2: ROS-Unity Integration"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Objective:"})," Implement bidirectional communication between ROS 2 and Unity for robot state visualization."]}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Task:"})," Create a Unity application that:"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Subscribes to ROS 2 topics for joint states and sensor data"}),"\n",(0,t.jsx)(i.li,{children:"Visualizes the robot's current pose and sensor readings"}),"\n",(0,t.jsx)(i.li,{children:"Publishes commands to ROS 2 for robot control"}),"\n",(0,t.jsx)(i.li,{children:"Handles connection management and error recovery"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Success Criteria:"})}),"\n",(0,t.jsxs)(i.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Successful ROS-Unity communication"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Real-time robot state visualization"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Command publishing functionality"]}),"\n",(0,t.jsxs)(i.li,{className:"task-list-item",children:[(0,t.jsx)(i.input,{type:"checkbox",disabled:!0})," ","Robust error handling"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Question:"})," What are the main advantages of using Unity for robotics visualization?\n",(0,t.jsx)(i.strong,{children:"Answer:"})," Unity provides high-quality graphics rendering, sophisticated interaction systems, VR/AR support, extensive asset ecosystem, and real-time visualization capabilities that enhance robot development and human-robot interaction."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Question:"})," How does Unity-ROS integration enable advanced robotics applications?\n",(0,t.jsx)(i.strong,{children:"Answer:"})," Unity-ROS integration enables bidirectional communication, allowing Unity to visualize real robot data while sending commands back to the robot, creating immersive environments for testing, training, and human-robot interaction."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Question:"})," What are important considerations for Unity performance in robotics applications?\n",(0,t.jsx)(i.strong,{children:"Answer:"})," Important considerations include maintaining high frame rates for real-time interaction, efficient rendering techniques, proper resource management, optimized physics simulation, and network communication efficiency."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"summary-and-key-takeaways",children:"Summary and Key Takeaways"}),"\n",(0,t.jsx)(i.h3,{id:"key-concepts-recap",children:"Key Concepts Recap"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Unity Robotics"}),": High-quality visualization and interaction platform for robotics applications"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"ROS Integration"}),": Bidirectional communication between Unity and ROS 2 systems"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Visualization Techniques"}),": Advanced rendering for robot state, sensor data, and environment"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Interaction Systems"}),": Human-robot interfaces and control systems"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Performance Optimization"}),": Balancing visual quality with real-time performance"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"practical-applications",children:"Practical Applications"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Training Environments"}),": Photorealistic simulation for AI development"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Human-Robot Interaction"}),": Intuitive interfaces for robot control and monitoring"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Data Visualization"}),": Real-time display of sensor data and robot metrics"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Remote Operation"}),": VR/AR interfaces for teleoperation"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Module Progression:"})," Next chapter covers ",(0,t.jsx)(i.a,{href:"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/ros2-integration",children:"ROS 2 Integration"})," for connecting simulation systems"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Further Reading:"})," Explore Unity ML-Agents for robot learning applications"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Practice Opportunities:"})," Implement Unity visualization for your robot platform"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"common-mistakes-and-troubleshooting",children:"Common Mistakes and Troubleshooting"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Mistake 1:"})," Poor performance due to heavy graphics \u2192 ",(0,t.jsx)(i.strong,{children:"Solution:"})," Optimize rendering and use LOD systems"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Mistake 2:"})," Unreliable ROS communication \u2192 ",(0,t.jsx)(i.strong,{children:"Solution:"})," Implement proper error handling and connection management"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Mistake 3:"})," Coordinate system mismatches \u2192 ",(0,t.jsx)(i.strong,{children:"Solution:"})," Carefully map between Unity and ROS coordinate systems"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"references-and-resources",children:"References and Resources"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://github.com/Unity-Technologies/Unity-Robotics-Hub",children:"Unity Robotics Hub"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://github.com/Unity-Technologies/ROS-TCP-Connector",children:"Unity-ROS Bridge Documentation"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://github.com/Unity-Technologies/ml-agents",children:"Unity ML-Agents Toolkit"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"https://github.com/Unity-Technologies/com.unity.perception",children:"Unity Perception Package"})}),"\n"]})]})}function p(n={}){const{wrapper:i}={...(0,o.R)(),...n.components};return i?(0,t.jsx)(i,{...n,children:(0,t.jsx)(h,{...n})}):h(n)}}}]);