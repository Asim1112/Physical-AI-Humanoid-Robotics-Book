"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[4911],{2327(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var i=t(4848),o=t(8453);const a={},s="Language and Intent Understanding",r={id:"module-4-vla/language-intent",title:"Language and Intent Understanding",description:"Overview",source:"@site/docs/module-4-vla/language-intent.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/language-intent",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/language-intent",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-4-vla/language-intent.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Vision-Language-Action Paradigm",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vla-paradigm"},next:{title:"Vision Grounding for Language Commands",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vision-grounding"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Language Processing Pipeline",id:"language-processing-pipeline",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Intent Recognition",id:"intent-recognition",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Slot Filling and Entity Recognition",id:"slot-filling-and-entity-recognition",level:3},{value:"Semantic Parsing",id:"semantic-parsing",level:2},{value:"Command to Action Mapping",id:"command-to-action-mapping",level:3},{value:"Contextual Understanding",id:"contextual-understanding",level:2},{value:"Dialog Management",id:"dialog-management",level:3},{value:"Language Grounding",id:"language-grounding",level:2},{value:"Spatial Language Understanding",id:"spatial-language-understanding",level:3},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:2},{value:"Clarification Strategies",id:"clarification-strategies",level:3},{value:"Best Practices",id:"best-practices",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"language-and-intent-understanding",children:"Language and Intent Understanding"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"Natural language understanding is a critical component of VLA systems, enabling humanoid robots to interpret human commands and infer intended actions. This chapter explores how language models process instructions, extract semantic meaning, and translate linguistic intent into executable robot behaviors."}),"\n",(0,i.jsx)(e.h2,{id:"language-processing-pipeline",children:"Language Processing Pipeline"}),"\n",(0,i.jsx)(e.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              LANGUAGE UNDERSTANDING PIPELINE                 \u2502\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  TEXT    \u2502\u2500\u2500\u25ba\u2502  TOKEN   \u2502\u2500\u2500\u25ba\u2502  SEMANTIC\u2502\u2500\u2500\u25ba\u2502  INTENT \u2502 \u2502\n\u2502  \u2502  INPUT   \u2502   \u2502  ENCODING\u2502   \u2502  PARSING \u2502   \u2502  EXTRACT\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502              \u2502               \u2502              \u2502       \u2502\n\u2502       \u2502              \u2502               \u2502              \u25bc       \u2502\n\u2502       \u2502              \u2502               \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502       \u2502              \u2502               \u2502         \u2502  ACTION \u2502 \u2502\n\u2502       \u2502              \u2502               \u2502         \u2502  PARAMS \u2502 \u2502\n\u2502       \u2502              \u2502               \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502              \u2502               \u25bc                      \u2502\n\u2502       \u2502              \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502       \u2502              \u2502         \u2502  ENTITY  \u2502                \u2502\n\u2502       \u2502              \u2502         \u2502  RECOG.  \u2502                \u2502\n\u2502       \u2502              \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502       \u2502              \u25bc                                      \u2502\n\u2502       \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n\u2502       \u2502         \u2502 ATTENTION\u2502                               \u2502\n\u2502       \u2502         \u2502  CONTEXT \u2502                               \u2502\n\u2502       \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u2502       \u25bc                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502  \u2502 LANGUAGE \u2502                                              \u2502\n\u2502  \u2502  MEMORY  \u2502                                              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h2,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,i.jsx)(e.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,i.jsx)(e.p,{children:"Classify user commands into predefined intent categories:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\nfrom typing import List, Dict, Tuple\n\nclass IntentClassifier(nn.Module):\n    \"\"\"\n    Intent classifier for robot commands.\n    Maps natural language to predefined action intents.\n    \"\"\"\n\n    def __init__(self, num_intents=20, hidden_dim=768):\n        super().__init__()\n\n        # Load pre-trained BERT\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_intents)\n        )\n\n        # Intent labels\n        self.intent_labels = [\n            'pick_and_place',\n            'navigate_to',\n            'follow_person',\n            'hand_object',\n            'open_door',\n            'close_door',\n            'push_button',\n            'turn_on',\n            'turn_off',\n            'wave_hand',\n            'nod_head',\n            'shake_head',\n            'point_at',\n            'look_at',\n            'wait',\n            'stop',\n            'return_home',\n            'follow_trajectory',\n            'grasp_object',\n            'release_object'\n        ]\n\n    def forward(self, text_inputs: List[str]) -> torch.Tensor:\n        \"\"\"\n        Classify intent from text input.\n\n        Args:\n            text_inputs: List of command strings\n\n        Returns:\n            Intent logits [batch_size, num_intents]\n        \"\"\"\n        # Tokenize\n        encoding = self.tokenizer(\n            text_inputs,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].to(self.bert.device)\n        attention_mask = encoding['attention_mask'].to(self.bert.device)\n\n        # BERT encoding\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n\n        # Classification\n        logits = self.classifier(pooled_output)\n\n        return logits\n\n    def predict_intent(self, text: str, return_confidence: bool = True) -> Dict:\n        \"\"\"\n        Predict intent from a single text command.\n\n        Args:\n            text: Command string\n            return_confidence: Whether to return confidence scores\n\n        Returns:\n            Dictionary with intent and optional confidence\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            logits = self.forward([text])\n            probs = torch.softmax(logits, dim=-1)\n            intent_idx = torch.argmax(probs, dim=-1).item()\n            confidence = probs[0, intent_idx].item()\n\n        result = {\n            'intent': self.intent_labels[intent_idx],\n            'intent_idx': intent_idx\n        }\n\n        if return_confidence:\n            result['confidence'] = confidence\n            # Get top-3 intents\n            top_probs, top_indices = torch.topk(probs[0], k=3)\n            result['top_intents'] = [\n                {\n                    'intent': self.intent_labels[idx.item()],\n                    'confidence': prob.item()\n                }\n                for idx, prob in zip(top_indices, top_probs)\n            ]\n\n        return result\n"})}),"\n",(0,i.jsx)(e.h3,{id:"slot-filling-and-entity-recognition",children:"Slot Filling and Entity Recognition"}),"\n",(0,i.jsx)(e.p,{children:"Extract specific parameters from commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from transformers import pipeline\n\nclass SlotFiller(nn.Module):\n    \"\"\"\n    Extract entities and parameters from natural language commands.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Named Entity Recognition model\n        self.ner_pipeline = pipeline(\n            \"ner\",\n            model=\"dslim/bert-base-NER\",\n            aggregation_strategy=\"simple\"\n        )\n\n        # Slot definitions\n        self.slot_types = {\n            'object': ['OBJECT', 'THING'],\n            'location': ['LOCATION', 'PLACE'],\n            'person': ['PERSON'],\n            'color': ['COLOR'],\n            'direction': ['DIRECTION'],\n            'quantity': ['NUMBER', 'QUANTITY']\n        }\n\n    def extract_slots(self, text: str) -> Dict[str, List[Dict]]:\n        \"\"\"\n        Extract slot values from text.\n\n        Args:\n            text: Input command\n\n        Returns:\n            Dictionary mapping slot types to extracted entities\n        \"\"\"\n        # Run NER\n        entities = self.ner_pipeline(text)\n\n        # Organize by slot type\n        slots = {slot_type: [] for slot_type in self.slot_types.keys()}\n\n        for entity in entities:\n            entity_type = entity['entity_group']\n            entity_text = entity['word']\n            confidence = entity['score']\n\n            # Map entity type to slot type\n            for slot_type, entity_types in self.slot_types.items():\n                if entity_type in entity_types:\n                    slots[slot_type].append({\n                        'value': entity_text,\n                        'confidence': confidence,\n                        'start': entity['start'],\n                        'end': entity['end']\n                    })\n\n        # Additional rule-based extraction\n        slots = self._rule_based_extraction(text, slots)\n\n        return slots\n\n    def _rule_based_extraction(self, text: str, slots: Dict) -> Dict:\n        \"\"\"Apply rule-based extraction for common patterns.\"\"\"\n        import re\n\n        # Extract colors\n        colors = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'orange', 'purple']\n        for color in colors:\n            if color in text.lower():\n                slots['color'].append({\n                    'value': color,\n                    'confidence': 1.0,\n                    'source': 'rule-based'\n                })\n\n        # Extract directions\n        directions = ['left', 'right', 'forward', 'backward', 'up', 'down', 'north', 'south', 'east', 'west']\n        for direction in directions:\n            if direction in text.lower():\n                slots['direction'].append({\n                    'value': direction,\n                    'confidence': 1.0,\n                    'source': 'rule-based'\n                })\n\n        # Extract quantities\n        numbers = re.findall(r'\\b\\d+\\b', text)\n        for number in numbers:\n            slots['quantity'].append({\n                'value': int(number),\n                'confidence': 1.0,\n                'source': 'rule-based'\n            })\n\n        return slots\n"})}),"\n",(0,i.jsx)(e.h2,{id:"semantic-parsing",children:"Semantic Parsing"}),"\n",(0,i.jsx)(e.h3,{id:"command-to-action-mapping",children:"Command to Action Mapping"}),"\n",(0,i.jsx)(e.p,{children:"Convert natural language commands to structured action representations:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from dataclasses import dataclass\nfrom typing import Optional, List, Any\n\n@dataclass\nclass ActionCommand:\n    \"\"\"Structured representation of a robot action command.\"\"\"\n    intent: str\n    object: Optional[str] = None\n    location: Optional[str] = None\n    target: Optional[str] = None\n    parameters: Optional[Dict[str, Any]] = None\n    constraints: Optional[List[str]] = None\n    confidence: float = 1.0\n\nclass SemanticParser:\n    \"\"\"\n    Parse natural language commands into structured action representations.\n    \"\"\"\n\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()\n        self.slot_filler = SlotFiller()\n\n        # Load pre-trained models\n        self.intent_classifier.eval()\n\n    def parse_command(self, text: str) -> ActionCommand:\n        \"\"\"\n        Parse natural language command into structured action.\n\n        Args:\n            text: Natural language command\n\n        Returns:\n            ActionCommand object\n        \"\"\"\n        # Classify intent\n        intent_result = self.intent_classifier.predict_intent(text)\n        intent = intent_result['intent']\n        confidence = intent_result['confidence']\n\n        # Extract slots\n        slots = self.slot_filler.extract_slots(text)\n\n        # Extract primary object\n        obj = None\n        if slots['object']:\n            obj = slots['object'][0]['value']\n\n        # Extract location\n        location = None\n        if slots['location']:\n            location = slots['location'][0]['value']\n\n        # Extract target (person or object)\n        target = None\n        if slots['person']:\n            target = slots['person'][0]['value']\n\n        # Build parameters dictionary\n        parameters = {}\n\n        if slots['color']:\n            parameters['color'] = slots['color'][0]['value']\n\n        if slots['direction']:\n            parameters['direction'] = slots['direction'][0]['value']\n\n        if slots['quantity']:\n            parameters['quantity'] = slots['quantity'][0]['value']\n\n        # Extract constraints from command\n        constraints = self._extract_constraints(text)\n\n        return ActionCommand(\n            intent=intent,\n            object=obj,\n            location=location,\n            target=target,\n            parameters=parameters if parameters else None,\n            constraints=constraints if constraints else None,\n            confidence=confidence\n        )\n\n    def _extract_constraints(self, text: str) -> List[str]:\n        \"\"\"Extract constraints from command text.\"\"\"\n        constraints = []\n\n        # Common constraint patterns\n        constraint_patterns = {\n            'carefully': 'gentle_motion',\n            'quickly': 'fast_motion',\n            'slowly': 'slow_motion',\n            'gently': 'gentle_motion',\n            'without touching': 'no_contact',\n            'avoid': 'obstacle_avoidance'\n        }\n\n        text_lower = text.lower()\n        for pattern, constraint in constraint_patterns.items():\n            if pattern in text_lower:\n                constraints.append(constraint)\n\n        return constraints\n\n    def command_to_robot_action(self, command: ActionCommand) -> Dict:\n        \"\"\"\n        Convert parsed command to robot-executable action.\n\n        Args:\n            command: Parsed ActionCommand\n\n        Returns:\n            Dictionary with robot action specification\n        \"\"\"\n        action_spec = {\n            'type': command.intent,\n            'confidence': command.confidence\n        }\n\n        # Map intent to robot primitives\n        if command.intent == 'pick_and_place':\n            action_spec['primitive'] = 'manipulation'\n            action_spec['grasp_object'] = command.object\n            action_spec['place_location'] = command.location\n\n        elif command.intent == 'navigate_to':\n            action_spec['primitive'] = 'navigation'\n            action_spec['target_location'] = command.location or command.object\n\n        elif command.intent == 'hand_object':\n            action_spec['primitive'] = 'handover'\n            action_spec['object'] = command.object\n            action_spec['recipient'] = command.target\n\n        elif command.intent in ['wave_hand', 'nod_head', 'point_at']:\n            action_spec['primitive'] = 'gesture'\n            action_spec['gesture_type'] = command.intent\n            action_spec['target'] = command.target or command.object\n\n        # Add parameters\n        if command.parameters:\n            action_spec['parameters'] = command.parameters\n\n        # Add constraints\n        if command.constraints:\n            action_spec['constraints'] = command.constraints\n\n        return action_spec\n"})}),"\n",(0,i.jsx)(e.h2,{id:"contextual-understanding",children:"Contextual Understanding"}),"\n",(0,i.jsx)(e.h3,{id:"dialog-management",children:"Dialog Management"}),"\n",(0,i.jsx)(e.p,{children:"Handle multi-turn conversations and context:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from collections import deque\n\nclass DialogManager:\n    \"\"\"\n    Manage conversational context for robot commands.\n    \"\"\"\n\n    def __init__(self, context_window=5):\n        self.context_window = context_window\n        self.conversation_history = deque(maxlen=context_window)\n        self.current_task = None\n        self.entities_in_context = {}\n\n    def process_utterance(self, text: str, semantic_parser: SemanticParser) -> ActionCommand:\n        \"\"\"\n        Process utterance with conversational context.\n\n        Args:\n            text: User utterance\n            semantic_parser: Semantic parser instance\n\n        Returns:\n            ActionCommand with context resolution\n        \"\"\"\n        # Parse the current utterance\n        command = semantic_parser.parse_command(text)\n\n        # Resolve references (pronouns, etc.)\n        command = self._resolve_references(command, text)\n\n        # Update conversation history\n        self.conversation_history.append({\n            'text': text,\n            'command': command,\n            'timestamp': time.time()\n        })\n\n        # Update entity context\n        self._update_entity_context(command)\n\n        # Update current task\n        if command.intent != 'wait' and command.intent != 'stop':\n            self.current_task = command\n\n        return command\n\n    def _resolve_references(self, command: ActionCommand, text: str) -> ActionCommand:\n        \"\"\"Resolve pronouns and references to previously mentioned entities.\"\"\"\n        text_lower = text.lower()\n\n        # Pronoun resolution\n        pronouns = ['it', 'that', 'this', 'them', 'those', 'these']\n\n        for pronoun in pronouns:\n            if pronoun in text_lower:\n                # Look for most recent relevant entity\n                if not command.object and 'object' in self.entities_in_context:\n                    command.object = self.entities_in_context['object']\n\n                if not command.location and 'location' in self.entities_in_context:\n                    command.location = self.entities_in_context['location']\n\n        # Relative location resolution\n        if command.location in ['there', 'here']:\n            if 'location' in self.entities_in_context:\n                command.location = self.entities_in_context['location']\n\n        return command\n\n    def _update_entity_context(self, command: ActionCommand):\n        \"\"\"Update entities in context based on command.\"\"\"\n        if command.object:\n            self.entities_in_context['object'] = command.object\n\n        if command.location:\n            self.entities_in_context['location'] = command.location\n\n        if command.target:\n            self.entities_in_context['target'] = command.target\n\n    def get_conversation_summary(self) -> str:\n        \"\"\"Get summary of recent conversation.\"\"\"\n        if not self.conversation_history:\n            return \"No conversation history.\"\n\n        summary = \"Recent conversation:\\n\"\n        for i, turn in enumerate(self.conversation_history):\n            summary += f\"{i+1}. User: {turn['text']}\\n\"\n            summary += f\"   Parsed: {turn['command'].intent}\"\n            if turn['command'].object:\n                summary += f\" (object: {turn['command'].object})\"\n            summary += \"\\n\"\n\n        return summary\n"})}),"\n",(0,i.jsx)(e.h2,{id:"language-grounding",children:"Language Grounding"}),"\n",(0,i.jsx)(e.h3,{id:"spatial-language-understanding",children:"Spatial Language Understanding"}),"\n",(0,i.jsx)(e.p,{children:"Interpret spatial relationships and references:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class SpatialLanguageGrounder:\n    \"\"\"\n    Ground spatial language to physical locations and relationships.\n    \"\"\"\n\n    def __init__(self):\n        # Spatial prepositions and their meanings\n        self.spatial_relations = {\n            'on': 'above_contact',\n            'above': 'above_no_contact',\n            'below': 'below',\n            'under': 'below_contact',\n            'in': 'inside',\n            'inside': 'inside',\n            'near': 'proximity',\n            'next to': 'adjacent',\n            'beside': 'adjacent',\n            'behind': 'behind',\n            'in front of': 'in_front',\n            'left of': 'left',\n            'right of': 'right',\n            'between': 'between'\n        }\n\n    def ground_spatial_expression(\n        self,\n        expression: str,\n        reference_objects: List[str],\n        scene_graph: Dict\n    ) -> Dict:\n        \"\"\"\n        Ground spatial expression to scene locations.\n\n        Args:\n            expression: Spatial expression (e.g., \"on the table\")\n            reference_objects: List of objects in the scene\n            scene_graph: Scene representation with object positions\n\n        Returns:\n            Grounded location specification\n        \"\"\"\n        # Extract spatial relation\n        relation = None\n        reference_object = None\n\n        for prep, rel_type in self.spatial_relations.items():\n            if prep in expression.lower():\n                relation = rel_type\n                # Extract reference object\n                parts = expression.lower().split(prep)\n                if len(parts) > 1:\n                    ref_text = parts[1].strip()\n                    # Match to actual object\n                    for obj in reference_objects:\n                        if obj.lower() in ref_text:\n                            reference_object = obj\n                            break\n                break\n\n        if not relation or not reference_object:\n            return {'status': 'failed', 'reason': 'Could not parse spatial expression'}\n\n        # Get reference object position from scene graph\n        if reference_object not in scene_graph:\n            return {'status': 'failed', 'reason': f'Object {reference_object} not found in scene'}\n\n        ref_position = scene_graph[reference_object]['position']\n        ref_bbox = scene_graph[reference_object].get('bbox', None)\n\n        # Calculate target location based on spatial relation\n        target_location = self._compute_spatial_location(\n            relation, ref_position, ref_bbox\n        )\n\n        return {\n            'status': 'success',\n            'relation': relation,\n            'reference_object': reference_object,\n            'target_location': target_location\n        }\n\n    def _compute_spatial_location(\n        self,\n        relation: str,\n        ref_position: List[float],\n        ref_bbox: Optional[Dict]\n    ) -> List[float]:\n        \"\"\"Compute target location based on spatial relation.\"\"\"\n        import numpy as np\n\n        x, y, z = ref_position\n\n        # Default offsets (in meters)\n        if relation == 'above_contact':\n            if ref_bbox:\n                z_offset = ref_bbox['height'] / 2 + 0.05  # 5cm above surface\n            else:\n                z_offset = 0.1\n            return [x, y, z + z_offset]\n\n        elif relation == 'above_no_contact':\n            return [x, y, z + 0.3]  # 30cm above\n\n        elif relation == 'below':\n            return [x, y, z - 0.3]\n\n        elif relation == 'proximity':\n            # Near but not specific direction - add small random offset\n            offset = np.random.uniform(-0.2, 0.2, size=2)\n            return [x + offset[0], y + offset[1], z]\n\n        elif relation == 'adjacent':\n            # To the side\n            return [x + 0.3, y, z]\n\n        elif relation == 'behind':\n            return [x, y - 0.3, z]\n\n        elif relation == 'in_front':\n            return [x, y + 0.3, z]\n\n        elif relation == 'left':\n            return [x - 0.3, y, z]\n\n        elif relation == 'right':\n            return [x + 0.3, y, z]\n\n        else:\n            return ref_position  # Default to reference position\n"})}),"\n",(0,i.jsx)(e.h2,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,i.jsx)(e.h3,{id:"clarification-strategies",children:"Clarification Strategies"}),"\n",(0,i.jsx)(e.p,{children:"Handle ambiguous commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class AmbiguityResolver:\n    \"\"\"\n    Resolve ambiguities in natural language commands.\n    \"\"\"\n\n    def __init__(self, confidence_threshold=0.7):\n        self.confidence_threshold = confidence_threshold\n\n    def check_ambiguity(self, command: ActionCommand, slots: Dict) -> Dict:\n        \"\"\"\n        Check for ambiguities in parsed command.\n\n        Args:\n            command: Parsed action command\n            slots: Extracted slots from slot filler\n\n        Returns:\n            Dictionary with ambiguity information\n        \"\"\"\n        ambiguities = []\n\n        # Check intent confidence\n        if command.confidence < self.confidence_threshold:\n            ambiguities.append({\n                'type': 'intent_uncertainty',\n                'message': f\"I'm not sure if you want me to {command.intent}\",\n                'confidence': command.confidence\n            })\n\n        # Check for multiple objects\n        if slots['object'] and len(slots['object']) > 1:\n            ambiguities.append({\n                'type': 'multiple_objects',\n                'message': f\"Which object do you mean: {', '.join([o['value'] for o in slots['object']])}?\",\n                'options': [o['value'] for o in slots['object']]\n            })\n\n        # Check for missing critical information\n        if command.intent == 'pick_and_place':\n            if not command.object:\n                ambiguities.append({\n                    'type': 'missing_object',\n                    'message': \"What should I pick up?\"\n                })\n            if not command.location:\n                ambiguities.append({\n                    'type': 'missing_location',\n                    'message': \"Where should I place it?\"\n                })\n\n        elif command.intent == 'navigate_to':\n            if not command.location and not command.object:\n                ambiguities.append({\n                    'type': 'missing_destination',\n                    'message': \"Where should I go?\"\n                })\n\n        return {\n            'has_ambiguity': len(ambiguities) > 0,\n            'ambiguities': ambiguities,\n            'count': len(ambiguities)\n        }\n\n    def generate_clarification_question(self, ambiguity: Dict) -> str:\n        \"\"\"Generate natural clarification question.\"\"\"\n        if 'message' in ambiguity:\n            return ambiguity['message']\n\n        amb_type = ambiguity['type']\n\n        templates = {\n            'intent_uncertainty': \"Could you clarify what you'd like me to do?\",\n            'multiple_objects': \"Which one do you mean?\",\n            'missing_object': \"What object should I work with?\",\n            'missing_location': \"Where should I move it?\",\n            'missing_destination': \"Where should I go?\"\n        }\n\n        return templates.get(amb_type, \"Could you please clarify?\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Robust Intent Recognition"}),": Train on diverse command variations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Awareness"}),": Maintain conversation history and entity tracking"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Graceful Degradation"}),": Handle low-confidence predictions with clarifications"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain Adaptation"}),": Fine-tune language models for robotics domain"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety Checks"}),": Validate commands before execution"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"User Feedback"}),": Learn from corrections and confirmations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-lingual Support"}),": Consider language diversity in deployment environments"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Language and intent understanding enables humanoid robots to interact naturally with humans, interpreting commands and translating them into executable actions with appropriate context and spatial grounding."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);