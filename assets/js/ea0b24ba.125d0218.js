"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[7390],{5864(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var o=t(4848),i=t(8453);const s={},r="Vision Grounding for Language Commands",a={id:"module-4-vla/vision-grounding",title:"Vision Grounding for Language Commands",description:"Overview",source:"@site/docs/module-4-vla/vision-grounding.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/vision-grounding",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vision-grounding",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-4-vla/vision-grounding.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Language and Intent Understanding",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/language-intent"},next:{title:"Multimodal Integration for Humanoid Robots",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/multimodal-integration"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Vision-Language Grounding Architecture",id:"vision-language-grounding-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Object Grounding",id:"object-grounding",level:2},{value:"CLIP-Based Object Grounding",id:"clip-based-object-grounding",level:3},{value:"Region-Based Grounding",id:"region-based-grounding",level:3},{value:"Spatial Grounding",id:"spatial-grounding",level:2},{value:"Spatial Relationship Understanding",id:"spatial-relationship-understanding",level:3},{value:"Visual Question Answering for Grounding",id:"visual-question-answering-for-grounding",level:2},{value:"VQA-Based Grounding",id:"vqa-based-grounding",level:3},{value:"Attention-Based Grounding",id:"attention-based-grounding",level:2},{value:"Cross-Modal Attention Visualization",id:"cross-modal-attention-visualization",level:3},{value:"Integration with Robot Actions",id:"integration-with-robot-actions",level:2},{value:"Grounding to Action Conversion",id:"grounding-to-action-conversion",level:3},{value:"Best Practices",id:"best-practices",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"vision-grounding-for-language-commands",children:"Vision Grounding for Language Commands"}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"Vision grounding is the process of connecting language references to visual perception, enabling robots to identify objects, locations, and relationships mentioned in natural language commands. This chapter explores techniques for grounding linguistic concepts in visual scenes, from object detection to spatial reasoning and visual question answering."}),"\n",(0,o.jsx)(e.h2,{id:"vision-language-grounding-architecture",children:"Vision-Language Grounding Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  VISION-LANGUAGE GROUNDING                      \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502  \u2502   VISUAL    \u2502          \u2502  LINGUISTIC \u2502                     \u2502\n\u2502  \u2502   INPUT     \u2502          \u2502   QUERY     \u2502                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502         \u2502                        \u2502                             \u2502\n\u2502         \u25bc                        \u25bc                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502  \u2502   VISION    \u2502          \u2502  LANGUAGE   \u2502                     \u2502\n\u2502  \u2502   ENCODER   \u2502          \u2502  ENCODER    \u2502                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502         \u2502                        \u2502                             \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n\u2502                     \u25bc                                           \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502            \u2502 CROSS-MODAL    \u2502                                  \u2502\n\u2502            \u2502 ATTENTION      \u2502                                  \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n\u2502                     \u2502                                           \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502         \u25bc                       \u25bc                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502  \u2502   OBJECT    \u2502         \u2502  SPATIAL    \u2502                      \u2502\n\u2502  \u2502   GROUNDING \u2502         \u2502  GROUNDING  \u2502                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502         \u2502                       \u2502                               \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                     \u25bc                                           \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502            \u2502  GROUNDED      \u2502                                  \u2502\n\u2502            \u2502  REFERENCES    \u2502                                  \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h2,{id:"object-grounding",children:"Object Grounding"}),"\n",(0,o.jsx)(e.h3,{id:"clip-based-object-grounding",children:"CLIP-Based Object Grounding"}),"\n",(0,o.jsx)(e.p,{children:"Use CLIP (Contrastive Language-Image Pre-training) for zero-shot object grounding:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport numpy as np\nfrom typing import List, Dict, Tuple\n\nclass CLIPObjectGrounder:\n    """\n    Ground language references to objects using CLIP.\n    """\n\n    def __init__(self, model_name="openai/clip-vit-base-patch32"):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Load CLIP model\n        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n\n        self.model.eval()\n\n    def ground_objects(\n        self,\n        image: Image.Image,\n        text_queries: List[str],\n        object_proposals: List[Dict]\n    ) -> List[Dict]:\n        """\n        Ground text queries to object proposals in image.\n\n        Args:\n            image: PIL Image\n            text_queries: List of text descriptions (e.g., ["red cup", "blue bottle"])\n            object_proposals: List of object bounding boxes with format\n                             {\'bbox\': [x, y, w, h], \'crop\': PIL.Image}\n\n        Returns:\n            List of grounded objects with confidence scores\n        """\n        grounded_objects = []\n\n        with torch.no_grad():\n            # Process text queries\n            text_inputs = self.processor(\n                text=text_queries,\n                return_tensors="pt",\n                padding=True\n            ).to(self.device)\n\n            text_features = self.model.get_text_features(**text_inputs)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n            # Process each object proposal\n            for prop in object_proposals:\n                # Get image features for this proposal\n                crop = prop[\'crop\']\n\n                image_inputs = self.processor(\n                    images=crop,\n                    return_tensors="pt"\n                ).to(self.device)\n\n                image_features = self.model.get_image_features(**image_inputs)\n                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n                # Compute similarity with each text query\n                similarities = (image_features @ text_features.T).squeeze(0)\n\n                # Find best matching query\n                best_match_idx = torch.argmax(similarities).item()\n                best_score = similarities[best_match_idx].item()\n\n                grounded_objects.append({\n                    \'bbox\': prop[\'bbox\'],\n                    \'text_query\': text_queries[best_match_idx],\n                    \'confidence\': best_score,\n                    \'all_scores\': similarities.cpu().numpy().tolist()\n                })\n\n        # Sort by confidence\n        grounded_objects.sort(key=lambda x: x[\'confidence\'], reverse=True)\n\n        return grounded_objects\n\n    def zero_shot_detection(\n        self,\n        image: Image.Image,\n        text_query: str,\n        object_proposals: List[Dict],\n        threshold: float = 0.25\n    ) -> List[Dict]:\n        """\n        Zero-shot object detection using text query.\n\n        Args:\n            image: PIL Image\n            text_query: Text description of target object\n            object_proposals: List of object proposals\n            threshold: Confidence threshold\n\n        Returns:\n            List of detected objects matching the query\n        """\n        grounded = self.ground_objects(image, [text_query], object_proposals)\n\n        # Filter by threshold\n        detections = [\n            obj for obj in grounded\n            if obj[\'confidence\'] > threshold and obj[\'text_query\'] == text_query\n        ]\n\n        return detections\n'})}),"\n",(0,o.jsx)(e.h3,{id:"region-based-grounding",children:"Region-Based Grounding"}),"\n",(0,o.jsx)(e.p,{children:"Ground references to specific image regions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class RegionGrounder:\n    \"\"\"\n    Ground linguistic references to specific image regions.\n    \"\"\"\n\n    def __init__(self):\n        # Object detection model for generating proposals\n        from transformers import DetrImageProcessor, DetrForObjectDetection\n\n        self.processor = DetrImageProcessor.from_pretrained(\n            \"facebook/detr-resnet-50\"\n        )\n        self.detector = DetrForObjectDetection.from_pretrained(\n            \"facebook/detr-resnet-50\"\n        )\n\n        self.clip_grounder = CLIPObjectGrounder()\n\n    def generate_object_proposals(\n        self,\n        image: Image.Image,\n        confidence_threshold: float = 0.7\n    ) -> List[Dict]:\n        \"\"\"\n        Generate object proposals from image.\n\n        Args:\n            image: PIL Image\n            confidence_threshold: Minimum detection confidence\n\n        Returns:\n            List of object proposals\n        \"\"\"\n        # Prepare image\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n\n        # Run detection\n        with torch.no_grad():\n            outputs = self.detector(**inputs)\n\n        # Post-process detections\n        target_sizes = torch.tensor([image.size[::-1]])\n        results = self.processor.post_process_object_detection(\n            outputs,\n            target_sizes=target_sizes,\n            threshold=confidence_threshold\n        )[0]\n\n        # Extract proposals\n        proposals = []\n        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n            bbox = box.cpu().numpy()\n            x, y, x2, y2 = bbox\n\n            # Crop region\n            crop = image.crop((x, y, x2, y2))\n\n            proposals.append({\n                'bbox': [x, y, x2 - x, y2 - y],  # [x, y, w, h]\n                'crop': crop,\n                'class_id': label.item(),\n                'confidence': score.item()\n            })\n\n        return proposals\n\n    def ground_text_to_region(\n        self,\n        image: Image.Image,\n        text_query: str\n    ) -> Dict:\n        \"\"\"\n        Ground text query to image region.\n\n        Args:\n            image: PIL Image\n            text_query: Text description\n\n        Returns:\n            Best matching region\n        \"\"\"\n        # Generate proposals\n        proposals = self.generate_object_proposals(image)\n\n        if not proposals:\n            return {'status': 'no_proposals', 'confidence': 0.0}\n\n        # Ground using CLIP\n        detections = self.clip_grounder.zero_shot_detection(\n            image, text_query, proposals\n        )\n\n        if not detections:\n            return {'status': 'no_match', 'confidence': 0.0}\n\n        # Return best match\n        best_match = detections[0]\n        return {\n            'status': 'success',\n            'bbox': best_match['bbox'],\n            'confidence': best_match['confidence'],\n            'query': text_query\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"spatial-grounding",children:"Spatial Grounding"}),"\n",(0,o.jsx)(e.h3,{id:"spatial-relationship-understanding",children:"Spatial Relationship Understanding"}),"\n",(0,o.jsx)(e.p,{children:"Ground spatial relationships mentioned in language:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class SpatialGrounder:\n    \"\"\"\n    Ground spatial relationships in visual scenes.\n    \"\"\"\n\n    def __init__(self):\n        self.region_grounder = RegionGrounder()\n\n    def compute_spatial_relationships(\n        self,\n        objects: List[Dict]\n    ) -> Dict[Tuple[int, int], List[str]]:\n        \"\"\"\n        Compute spatial relationships between objects.\n\n        Args:\n            objects: List of detected objects with bboxes\n\n        Returns:\n            Dictionary mapping object pairs to relationship labels\n        \"\"\"\n        relationships = {}\n\n        for i, obj1 in enumerate(objects):\n            for j, obj2 in enumerate(objects):\n                if i == j:\n                    continue\n\n                bbox1 = obj1['bbox']  # [x, y, w, h]\n                bbox2 = obj2['bbox']\n\n                # Compute relationships\n                rels = self._compute_pairwise_relationships(bbox1, bbox2)\n\n                if rels:\n                    relationships[(i, j)] = rels\n\n        return relationships\n\n    def _compute_pairwise_relationships(\n        self,\n        bbox1: List[float],\n        bbox2: List[float]\n    ) -> List[str]:\n        \"\"\"Compute relationships between two bounding boxes.\"\"\"\n        x1, y1, w1, h1 = bbox1\n        x2, y2, w2, h2 = bbox2\n\n        # Centers\n        cx1, cy1 = x1 + w1/2, y1 + h1/2\n        cx2, cy2 = x2 + w2/2, y2 + h2/2\n\n        relationships = []\n\n        # Above/Below\n        if cy1 < cy2 - h2/2:\n            relationships.append('above')\n        elif cy1 > cy2 + h2/2:\n            relationships.append('below')\n\n        # Left/Right\n        if cx1 < cx2 - w2/2:\n            relationships.append('left_of')\n        elif cx1 > cx2 + w2/2:\n            relationships.append('right_of')\n\n        # Proximity\n        distance = np.sqrt((cx1 - cx2)**2 + (cy1 - cy2)**2)\n        max_dist = max(w1, w2, h1, h2)\n\n        if distance < max_dist * 1.5:\n            relationships.append('near')\n\n        # Containment (overlap)\n        if self._check_overlap(bbox1, bbox2):\n            overlap_ratio = self._compute_overlap_ratio(bbox1, bbox2)\n            if overlap_ratio > 0.7:\n                relationships.append('overlapping')\n\n        return relationships\n\n    def _check_overlap(self, bbox1: List[float], bbox2: List[float]) -> bool:\n        \"\"\"Check if two bboxes overlap.\"\"\"\n        x1, y1, w1, h1 = bbox1\n        x2, y2, w2, h2 = bbox2\n\n        return not (x1 + w1 < x2 or x2 + w2 < x1 or\n                   y1 + h1 < y2 or y2 + h2 < y1)\n\n    def _compute_overlap_ratio(self, bbox1: List[float], bbox2: List[float]) -> float:\n        \"\"\"Compute IoU between two bboxes.\"\"\"\n        x1, y1, w1, h1 = bbox1\n        x2, y2, w2, h2 = bbox2\n\n        # Intersection\n        x_overlap = max(0, min(x1 + w1, x2 + w2) - max(x1, x2))\n        y_overlap = max(0, min(y1 + h1, y2 + h2) - max(y1, y2))\n        intersection = x_overlap * y_overlap\n\n        # Union\n        area1 = w1 * h1\n        area2 = w2 * h2\n        union = area1 + area2 - intersection\n\n        return intersection / union if union > 0 else 0.0\n\n    def ground_spatial_query(\n        self,\n        image: Image.Image,\n        query: str\n    ) -> Dict:\n        \"\"\"\n        Ground spatial query like \"the cup on the table\".\n\n        Args:\n            image: PIL Image\n            query: Spatial query\n\n        Returns:\n            Grounded result with object and spatial relationship\n        \"\"\"\n        # Parse query to extract object and spatial relation\n        # This is simplified - would use semantic parser in practice\n        parts = query.lower().split(' on ')\n\n        if len(parts) != 2:\n            # Try other prepositions\n            for prep in [' above ', ' below ', ' next to ', ' near ']:\n                if prep in query.lower():\n                    parts = query.lower().split(prep)\n                    break\n\n        if len(parts) != 2:\n            return {'status': 'failed', 'reason': 'Could not parse spatial query'}\n\n        target_obj = parts[0].strip()\n        reference_obj = parts[1].strip()\n\n        # Detect all objects\n        proposals = self.region_grounder.generate_object_proposals(image)\n\n        # Ground both objects\n        target_detections = self.region_grounder.clip_grounder.zero_shot_detection(\n            image, target_obj, proposals\n        )\n        reference_detections = self.region_grounder.clip_grounder.zero_shot_detection(\n            image, reference_obj, proposals\n        )\n\n        if not target_detections or not reference_detections:\n            return {'status': 'failed', 'reason': 'Could not find objects'}\n\n        # Find objects with correct spatial relationship\n        objects = target_detections + reference_detections\n        relationships = self.compute_spatial_relationships(objects)\n\n        # Find matching pair\n        for (i, j), rels in relationships.items():\n            if 'above' in rels or 'on' in rels:  # Simplified\n                return {\n                    'status': 'success',\n                    'target': target_detections[i] if i < len(target_detections) else None,\n                    'reference': reference_detections[j - len(target_detections)] if j >= len(target_detections) else None,\n                    'relationship': rels\n                }\n\n        return {'status': 'failed', 'reason': 'No matching spatial relationship found'}\n"})}),"\n",(0,o.jsx)(e.h2,{id:"visual-question-answering-for-grounding",children:"Visual Question Answering for Grounding"}),"\n",(0,o.jsx)(e.h3,{id:"vqa-based-grounding",children:"VQA-Based Grounding"}),"\n",(0,o.jsx)(e.p,{children:"Use visual question answering to verify grounding:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from transformers import ViltProcessor, ViltForQuestionAnswering\n\nclass VQAGroundingVerifier:\n    """\n    Verify grounding results using visual question answering.\n    """\n\n    def __init__(self):\n        self.processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")\n        self.model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")\n\n    def verify_object_presence(\n        self,\n        image: Image.Image,\n        object_name: str\n    ) -> Dict:\n        """\n        Verify if object is present in image using VQA.\n\n        Args:\n            image: PIL Image\n            object_name: Name of object to verify\n\n        Returns:\n            Verification result with confidence\n        """\n        # Ask "Is there a {object_name}?"\n        question = f"Is there a {object_name}?"\n\n        # Process inputs\n        inputs = self.processor(image, question, return_tensors="pt")\n\n        # Get answer\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        logits = outputs.logits\n        idx = logits.argmax(-1).item()\n        answer = self.model.config.id2label[idx]\n\n        confidence = torch.softmax(logits, dim=-1)[0, idx].item()\n\n        return {\n            \'present\': answer.lower() in [\'yes\', \'true\'],\n            \'answer\': answer,\n            \'confidence\': confidence,\n            \'question\': question\n        }\n\n    def verify_spatial_relationship(\n        self,\n        image: Image.Image,\n        object1: str,\n        relation: str,\n        object2: str\n    ) -> Dict:\n        """\n        Verify spatial relationship using VQA.\n\n        Args:\n            image: PIL Image\n            object1: First object\n            relation: Spatial relation (\'on\', \'above\', \'next to\', etc.)\n            object2: Second object\n\n        Returns:\n            Verification result\n        """\n        question = f"Is the {object1} {relation} the {object2}?"\n\n        inputs = self.processor(image, question, return_tensors="pt")\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        logits = outputs.logits\n        idx = logits.argmax(-1).item()\n        answer = self.model.config.id2label[idx]\n\n        confidence = torch.softmax(logits, dim=-1)[0, idx].item()\n\n        return {\n            \'verified\': answer.lower() in [\'yes\', \'true\'],\n            \'answer\': answer,\n            \'confidence\': confidence,\n            \'question\': question\n        }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"attention-based-grounding",children:"Attention-Based Grounding"}),"\n",(0,o.jsx)(e.h3,{id:"cross-modal-attention-visualization",children:"Cross-Modal Attention Visualization"}),"\n",(0,o.jsx)(e.p,{children:"Visualize where the model attends when grounding:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass AttentionVisualizer:\n    \"\"\"\n    Visualize cross-modal attention for grounding.\n    \"\"\"\n\n    def __init__(self, vla_model):\n        self.model = vla_model\n\n    def get_attention_maps(\n        self,\n        image: torch.Tensor,\n        text: str\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Extract attention maps from VLA model.\n\n        Args:\n            image: Image tensor [3, H, W]\n            text: Text query\n\n        Returns:\n            Dictionary of attention maps\n        \"\"\"\n        self.model.eval()\n\n        with torch.no_grad():\n            # Forward pass with attention tracking\n            image_batch = image.unsqueeze(0)\n            text_batch = [text]\n\n            # Get vision features\n            vision_features = self.model.vision_encoder(image_batch)\n\n            # Get language features\n            language_features = self.model.language_encoder(text_batch)\n\n            # Get attention weights from fusion module\n            # This assumes the fusion module exposes attention weights\n            fusion_output = self.model.fusion(vision_features, language_features)\n\n            # Extract attention maps\n            attention_maps = {}\n\n            if hasattr(self.model.fusion, 'get_attention_weights'):\n                attention_weights = self.model.fusion.get_attention_weights()\n                attention_maps['cross_modal'] = attention_weights\n\n        return attention_maps\n\n    def visualize_grounding(\n        self,\n        image: Image.Image,\n        text: str,\n        grounding_result: Dict\n    ):\n        \"\"\"\n        Visualize grounding result on image.\n\n        Args:\n            image: PIL Image\n            text: Text query\n            grounding_result: Grounding result with bbox\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as patches\n\n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n        # Display image\n        ax.imshow(image)\n\n        # Draw bounding box\n        if 'bbox' in grounding_result:\n            bbox = grounding_result['bbox']\n            x, y, w, h = bbox\n\n            rect = patches.Rectangle(\n                (x, y), w, h,\n                linewidth=3,\n                edgecolor='red',\n                facecolor='none'\n            )\n            ax.add_patch(rect)\n\n            # Add label\n            label = f\"{text}\\nConf: {grounding_result.get('confidence', 0):.2f}\"\n            ax.text(\n                x, y - 10,\n                label,\n                color='white',\n                backgroundcolor='red',\n                fontsize=12\n            )\n\n        ax.axis('off')\n        ax.set_title(f\"Grounded: '{text}'\", fontsize=14)\n\n        plt.tight_layout()\n        plt.show()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-robot-actions",children:"Integration with Robot Actions"}),"\n",(0,o.jsx)(e.h3,{id:"grounding-to-action-conversion",children:"Grounding to Action Conversion"}),"\n",(0,o.jsx)(e.p,{children:"Convert grounded references to robot actions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class GroundingToActionConverter:\n    \"\"\"\n    Convert grounded visual references to robot actions.\n    \"\"\"\n\n    def __init__(self):\n        self.region_grounder = RegionGrounder()\n        self.spatial_grounder = SpatialGrounder()\n\n    def ground_and_act(\n        self,\n        image: Image.Image,\n        command: str,\n        camera_intrinsics: Dict\n    ) -> Dict:\n        \"\"\"\n        Ground command and convert to robot action.\n\n        Args:\n            image: Current camera image\n            command: Natural language command\n            camera_intrinsics: Camera parameters for 3D projection\n\n        Returns:\n            Robot action specification\n        \"\"\"\n        # Ground the command to visual elements\n        grounding_result = self.region_grounder.ground_text_to_region(image, command)\n\n        if grounding_result['status'] != 'success':\n            return {\n                'success': False,\n                'error': 'Failed to ground command to visual scene'\n            }\n\n        # Convert 2D bbox to 3D position estimate\n        bbox = grounding_result['bbox']\n        position_3d = self.estimate_3d_position(bbox, camera_intrinsics)\n\n        # Create action specification\n        action_spec = {\n            'success': True,\n            'action_type': 'reach',\n            'target_position': position_3d,\n            'target_bbox': bbox,\n            'confidence': grounding_result['confidence'],\n            'grounded_object': command\n        }\n\n        return action_spec\n\n    def estimate_3d_position(\n        self,\n        bbox_2d: List[float],\n        camera_intrinsics: Dict,\n        assumed_depth: float = 1.0\n    ) -> List[float]:\n        \"\"\"\n        Estimate 3D position from 2D bbox and camera parameters.\n\n        Args:\n            bbox_2d: 2D bounding box [x, y, w, h]\n            camera_intrinsics: Camera parameters (fx, fy, cx, cy)\n            assumed_depth: Assumed depth in meters\n\n        Returns:\n            Estimated 3D position [x, y, z]\n        \"\"\"\n        x, y, w, h = bbox_2d\n\n        # Center of bbox\n        u = x + w / 2\n        v = y + h / 2\n\n        # Camera intrinsics\n        fx = camera_intrinsics['fx']\n        fy = camera_intrinsics['fy']\n        cx = camera_intrinsics['cx']\n        cy = camera_intrinsics['cy']\n\n        # Back-project to 3D\n        z = assumed_depth\n        x_3d = (u - cx) * z / fx\n        y_3d = (v - cy) * z / fy\n\n        return [x_3d, y_3d, z]\n"})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-Modal Pre-training"}),": Use models pre-trained on large vision-language datasets (CLIP, ALIGN)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Zero-Shot Capability"}),": Design systems that can ground novel object categories"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Spatial Reasoning"}),": Implement robust spatial relationship understanding"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Verification"}),": Use VQA or other methods to verify grounding results"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"3D Grounding"}),": Convert 2D visual grounding to 3D robot coordinates"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Attention Visualization"}),": Visualize attention to debug and improve grounding"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Handling"}),": Gracefully handle failed grounding attempts"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Vision grounding connects language understanding with visual perception, enabling robots to identify and locate objects and relationships mentioned in natural language commands."})]})}function u(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>a});var o=t(6540);const i={},s=o.createContext(i);function r(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);