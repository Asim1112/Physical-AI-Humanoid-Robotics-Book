"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[7832],{8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},8581(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var i=t(4848),o=t(8453);const s={},a="VLA Deployment for Humanoid Robots",r={id:"module-4-vla/vla-deployment",title:"VLA Deployment for Humanoid Robots",description:"Overview",source:"@site/docs/module-4-vla/vla-deployment.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/vla-deployment",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vla-deployment",draft:!1,unlisted:!1,editUrl:"https://github.com/Asim1112/Physical-AI-Humanoid-Robotics-Book/edit/main/frontend/docs/module-4-vla/vla-deployment.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Multimodal Integration for Humanoid Robots",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/multimodal-integration"},next:{title:"Module 4 Exercises: Vision-Language-Action (VLA)",permalink:"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/exercises"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Deployment Architecture",id:"deployment-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Model Optimization for Edge Deployment",id:"model-optimization-for-edge-deployment",level:2},{value:"TensorRT Optimization",id:"tensorrt-optimization",level:3},{value:"Quantization",id:"quantization",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"VLA ROS 2 Node",id:"vla-ros-2-node",level:3},{value:"Safety Systems",id:"safety-systems",level:2},{value:"Multi-Layer Safety Architecture",id:"multi-layer-safety-architecture",level:3},{value:"Monitoring and Telemetry",id:"monitoring-and-telemetry",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Best Practices for Deployment",id:"best-practices-for-deployment",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"vla-deployment-for-humanoid-robots",children:"VLA Deployment for Humanoid Robots"}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Deploying Vision-Language-Action (VLA) models on physical humanoid robots requires careful consideration of computational constraints, real-time performance, safety, and robustness. This chapter provides a comprehensive guide to deploying VLA systems in production environments."}),"\n",(0,i.jsx)(n.h2,{id:"deployment-architecture",children:"Deployment Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA DEPLOYMENT SYSTEM                        \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502   SENSOR     \u2502   \u2502   VLA MODEL  \u2502   \u2502   ROBOT      \u2502      \u2502\n\u2502  \u2502   INTERFACE  \u2502\u2500\u2500\u25ba\u2502   INFERENCE  \u2502\u2500\u2500\u25ba\u2502   CONTROL    \u2502      \u2502\n\u2502  \u2502              \u2502   \u2502              \u2502   \u2502              \u2502      \u2502\n\u2502  \u2502 \u2022 Cameras    \u2502   \u2502 \u2022 Vision Enc \u2502   \u2502 \u2022 Motion     \u2502      \u2502\n\u2502  \u2502 \u2022 IMU        \u2502   \u2502 \u2022 Lang Enc   \u2502   \u2502   Planning   \u2502      \u2502\n\u2502  \u2502 \u2022 Joints     \u2502   \u2502 \u2022 Fusion     \u2502   \u2502 \u2022 Safety     \u2502      \u2502\n\u2502  \u2502 \u2022 Force      \u2502   \u2502 \u2022 Action Dec \u2502   \u2502 \u2022 Execution  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502         \u2502                   \u2502                   \u2502              \u2502\n\u2502         \u2502                   \u25bc                   \u2502              \u2502\n\u2502         \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502              \u2502\n\u2502         \u2502          \u2502 OPTIMIZATION \u2502             \u2502              \u2502\n\u2502         \u2502          \u2502              \u2502             \u2502              \u2502\n\u2502         \u2502          \u2502 \u2022 TensorRT   \u2502             \u2502              \u2502\n\u2502         \u2502          \u2502 \u2022 Quantize   \u2502             \u2502              \u2502\n\u2502         \u2502          \u2502 \u2022 Pruning    \u2502             \u2502              \u2502\n\u2502         \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502              \u2502\n\u2502         \u2502                                       \u2502              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                         \u25bc                                       \u2502\n\u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                \u2502\n\u2502                \u2502   SAFETY &   \u2502                                \u2502\n\u2502                \u2502  MONITORING  \u2502                                \u2502\n\u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"model-optimization-for-edge-deployment",children:"Model Optimization for Edge Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"tensorrt-optimization",children:"TensorRT Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Optimize VLA models using NVIDIA TensorRT:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch_tensorrt\nfrom typing import List, Tuple\n\nclass VLATensorRTOptimizer:\n    """\n    Optimize VLA model for deployment using TensorRT.\n    """\n\n    def __init__(self, model_path: str):\n        self.model = torch.jit.load(model_path)\n        self.optimized_model = None\n\n    def optimize_for_inference(\n        self,\n        input_shapes: dict,\n        precision: str = "fp16",\n        workspace_size: int = 2 << 30  # 2GB\n    ):\n        """\n        Optimize model using TensorRT.\n\n        Args:\n            input_shapes: Dictionary of input shapes\n            precision: "fp32", "fp16", or "int8"\n            workspace_size: TensorRT workspace size in bytes\n        """\n        # Define input specifications\n        inputs = []\n\n        # Vision input\n        if \'image\' in input_shapes:\n            inputs.append(\n                torch_tensorrt.Input(\n                    min_shape=input_shapes[\'image\'][\'min\'],\n                    opt_shape=input_shapes[\'image\'][\'opt\'],\n                    max_shape=input_shapes[\'image\'][\'max\'],\n                    dtype=torch.float32\n                )\n            )\n\n        # Proprioception input\n        if \'proprio\' in input_shapes:\n            inputs.append(\n                torch_tensorrt.Input(\n                    min_shape=input_shapes[\'proprio\'][\'min\'],\n                    opt_shape=input_shapes[\'proprio\'][\'opt\'],\n                    max_shape=input_shapes[\'proprio\'][\'max\'],\n                    dtype=torch.float32\n                )\n            )\n\n        # Set precision\n        enabled_precisions = {torch.float32}\n        if precision == "fp16":\n            enabled_precisions.add(torch.float16)\n        elif precision == "int8":\n            enabled_precisions.add(torch.int8)\n\n        # Compile with TensorRT\n        self.optimized_model = torch_tensorrt.compile(\n            self.model,\n            inputs=inputs,\n            enabled_precisions=enabled_precisions,\n            workspace_size=workspace_size,\n            truncate_long_and_double=True,\n            device=torch.device("cuda:0")\n        )\n\n        print(f"Model optimized with TensorRT using {precision} precision")\n\n    def benchmark(self, sample_inputs: Tuple, num_iterations: int = 100):\n        """Benchmark optimized model."""\n        import time\n\n        # Warmup\n        for _ in range(10):\n            with torch.no_grad():\n                _ = self.optimized_model(*sample_inputs)\n\n        # Benchmark\n        torch.cuda.synchronize()\n        start = time.time()\n\n        for _ in range(num_iterations):\n            with torch.no_grad():\n                _ = self.optimized_model(*sample_inputs)\n\n        torch.cuda.synchronize()\n        end = time.time()\n\n        avg_latency = (end - start) / num_iterations * 1000  # ms\n        throughput = num_iterations / (end - start)  # samples/sec\n\n        return {\n            \'avg_latency_ms\': avg_latency,\n            \'throughput\': throughput,\n            \'iterations\': num_iterations\n        }\n\n    def save_optimized_model(self, save_path: str):\n        """Save optimized TensorRT model."""\n        if self.optimized_model is None:\n            raise ValueError("Model not optimized yet")\n\n        torch.jit.save(self.optimized_model, save_path)\n        print(f"Optimized model saved to {save_path}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"quantization",children:"Quantization"}),"\n",(0,i.jsx)(n.p,{children:"Reduce model size and improve inference speed:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch.quantization as quantization\n\nclass VLAQuantizer:\n    """\n    Quantize VLA model for efficient deployment.\n    """\n\n    def __init__(self, model):\n        self.model = model\n        self.quantized_model = None\n\n    def quantize_dynamic(self):\n        """\n        Apply dynamic quantization (weights only).\n        Best for models with large linear layers.\n        """\n        self.quantized_model = quantization.quantize_dynamic(\n            self.model,\n            {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},\n            dtype=torch.qint8\n        )\n\n        return self.quantized_model\n\n    def quantize_static(self, calibration_loader):\n        """\n        Apply static quantization (weights and activations).\n        Requires calibration data.\n        """\n        # Prepare model for quantization\n        self.model.qconfig = quantization.get_default_qconfig(\'fbgemm\')\n        quantization.prepare(self.model, inplace=True)\n\n        # Calibrate\n        self.model.eval()\n        with torch.no_grad():\n            for batch in calibration_loader:\n                self.model(batch[\'image\'], batch[\'text\'], batch[\'proprio\'])\n\n        # Convert to quantized model\n        self.quantized_model = quantization.convert(self.model, inplace=False)\n\n        return self.quantized_model\n\n    def compare_performance(self, test_data):\n        """Compare original and quantized model."""\n        import time\n\n        # Original model\n        start = time.time()\n        with torch.no_grad():\n            orig_output = self.model(**test_data)\n        orig_time = time.time() - start\n\n        # Quantized model\n        start = time.time()\n        with torch.no_grad():\n            quant_output = self.quantized_model(**test_data)\n        quant_time = time.time() - start\n\n        # Size comparison\n        orig_size = sum(p.numel() * p.element_size() for p in self.model.parameters())\n        quant_size = sum(p.numel() * p.element_size() for p in self.quantized_model.parameters())\n\n        return {\n            \'speedup\': orig_time / quant_time,\n            \'size_reduction\': orig_size / quant_size,\n            \'orig_latency_ms\': orig_time * 1000,\n            \'quant_latency_ms\': quant_time * 1000,\n            \'orig_size_mb\': orig_size / (1024 * 1024),\n            \'quant_size_mb\': quant_size / (1024 * 1024)\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.h3,{id:"vla-ros-2-node",children:"VLA ROS 2 Node"}),"\n",(0,i.jsx)(n.p,{children:"Create a ROS 2 node for VLA inference:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState, Imu\nfrom std_msgs.msg import String, Float64MultiArray\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport torch\nimport numpy as np\n\nclass VLANode(Node):\n    """\n    ROS 2 node for VLA model inference.\n    """\n\n    def __init__(self, model_path: str):\n        super().__init__(\'vla_node\')\n\n        # Load optimized model\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.model = torch.jit.load(model_path).to(self.device)\n        self.model.eval()\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Declare parameters\n        self.declare_parameter(\'inference_rate\', 10.0)\n        self.declare_parameter(\'action_scale\', 1.0)\n        self.declare_parameter(\'enable_safety_checks\', True)\n\n        # Get parameters\n        self.inference_rate = self.get_parameter(\'inference_rate\').value\n        self.action_scale = self.get_parameter(\'action_scale\').value\n        self.enable_safety = self.get_parameter(\'enable_safety_checks\').value\n\n        # State\n        self.current_image = None\n        self.current_text = "idle"\n        self.current_proprio = None\n        self.last_action = None\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10)\n        self.text_sub = self.create_subscription(\n            String, \'/vla/command\', self.text_callback, 10)\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10)\n\n        # Publishers\n        self.action_pub = self.create_publisher(\n            Float64MultiArray, \'/vla/action\', 10)\n        self.status_pub = self.create_publisher(\n            String, \'/vla/status\', 10)\n\n        # Timer for inference\n        self.inference_timer = self.create_timer(\n            1.0 / self.inference_rate, self.inference_step)\n\n        self.get_logger().info(\'VLA Node initialized\')\n\n    def image_callback(self, msg):\n        """Update current image."""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")\n            self.current_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f\'Image callback error: {e}\')\n\n    def text_callback(self, msg):\n        """Update current command."""\n        self.current_text = msg.data\n        self.get_logger().info(f\'Received command: {msg.data}\')\n\n    def joint_callback(self, msg):\n        """Update proprioceptive state."""\n        # Combine joint positions and velocities\n        positions = np.array(msg.position)\n        velocities = np.array(msg.velocity)\n\n        if len(velocities) < len(positions):\n            velocities = np.zeros_like(positions)\n\n        self.current_proprio = np.concatenate([positions, velocities])\n\n    def imu_callback(self, msg):\n        """Process IMU data."""\n        # Could incorporate IMU into proprioception\n        pass\n\n    def inference_step(self):\n        """Execute VLA inference step."""\n        if self.current_image is None or self.current_proprio is None:\n            return\n\n        try:\n            # Prepare inputs\n            image_tensor = self.preprocess_image(self.current_image)\n            proprio_tensor = torch.FloatTensor(self.current_proprio).unsqueeze(0).to(self.device)\n\n            # Inference\n            with torch.no_grad():\n                outputs = self.model(\n                    image_tensor,\n                    [self.current_text],\n                    proprio_tensor\n                )\n\n            # Extract action\n            action = outputs[\'actions\'][0].cpu().numpy()\n\n            # Scale action\n            action = action * self.action_scale\n\n            # Safety checks\n            if self.enable_safety:\n                action = self.apply_safety_checks(action)\n\n            # Publish action\n            action_msg = Float64MultiArray()\n            action_msg.data = action.tolist()\n            self.action_pub.publish(action_msg)\n\n            self.last_action = action\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f\'Executing: {self.current_text}\'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Inference error: {e}\')\n\n    def preprocess_image(self, image):\n        """Preprocess image for model input."""\n        import cv2\n        from torchvision import transforms\n\n        # Resize\n        image = cv2.resize(image, (224, 224))\n\n        # Convert to tensor and normalize\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        image_tensor = transform(image).unsqueeze(0).to(self.device)\n        return image_tensor\n\n    def apply_safety_checks(self, action):\n        """Apply safety constraints to action."""\n        # Joint limits\n        action = np.clip(action, -1.0, 1.0)\n\n        # Velocity limits\n        if self.last_action is not None:\n            max_delta = 0.1  # Maximum change per step\n            delta = action - self.last_action\n            delta = np.clip(delta, -max_delta, max_delta)\n            action = self.last_action + delta\n\n        return action\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vla_node = VLANode(model_path=\'/path/to/optimized_vla_model.pt\')\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        vla_node.get_logger().info(\'VLA Node stopped\')\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"safety-systems",children:"Safety Systems"}),"\n",(0,i.jsx)(n.h3,{id:"multi-layer-safety-architecture",children:"Multi-Layer Safety Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Implement comprehensive safety checks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class VLASafetySystem:\n    \"\"\"\n    Multi-layer safety system for VLA deployment.\n    \"\"\"\n\n    def __init__(self):\n        # Safety thresholds\n        self.joint_limits = self.load_joint_limits()\n        self.velocity_limits = {'max': 1.0, 'max_delta': 0.1}\n        self.force_threshold = 50.0  # Newtons\n        self.workspace_bounds = {\n            'x': (-1.0, 1.0),\n            'y': (-1.0, 1.0),\n            'z': (0.0, 2.0)\n        }\n\n        # Safety state\n        self.emergency_stop_active = False\n        self.safety_violations = []\n\n    def validate_action(\n        self,\n        action: np.ndarray,\n        current_state: dict,\n        predicted_outcome: dict\n    ) -> dict:\n        \"\"\"\n        Validate action before execution.\n\n        Args:\n            action: Proposed robot action\n            current_state: Current robot state\n            predicted_outcome: Predicted result of action\n\n        Returns:\n            Validation result with safe action\n        \"\"\"\n        violations = []\n        safe_action = action.copy()\n\n        # Check joint limits\n        if not self._check_joint_limits(action, current_state):\n            violations.append('joint_limits')\n            safe_action = self._clip_to_joint_limits(safe_action, current_state)\n\n        # Check velocity limits\n        if not self._check_velocity_limits(action, current_state):\n            violations.append('velocity_limits')\n            safe_action = self._limit_velocity(safe_action, current_state)\n\n        # Check workspace bounds\n        if not self._check_workspace(predicted_outcome):\n            violations.append('workspace_bounds')\n            safe_action = self._adjust_for_workspace(safe_action)\n\n        # Check collision prediction\n        if self._predict_collision(predicted_outcome):\n            violations.append('collision_risk')\n            safe_action = self._avoid_collision(safe_action, predicted_outcome)\n\n        # Check stability\n        if not self._check_stability(predicted_outcome):\n            violations.append('stability_risk')\n            safe_action = np.zeros_like(action)  # Stop if unstable\n\n        return {\n            'is_safe': len(violations) == 0,\n            'violations': violations,\n            'safe_action': safe_action,\n            'original_action': action\n        }\n\n    def _check_joint_limits(self, action, state):\n        \"\"\"Check if action respects joint limits.\"\"\"\n        current_positions = state['joint_positions']\n        future_positions = current_positions + action\n\n        for i, (pos, limits) in enumerate(zip(future_positions, self.joint_limits)):\n            if pos < limits['min'] or pos > limits['max']:\n                return False\n        return True\n\n    def _check_velocity_limits(self, action, state):\n        \"\"\"Check velocity constraints.\"\"\"\n        last_action = state.get('last_action', np.zeros_like(action))\n        delta = np.abs(action - last_action)\n        return np.all(delta <= self.velocity_limits['max_delta'])\n\n    def _check_workspace(self, outcome):\n        \"\"\"Check if end-effector stays in workspace.\"\"\"\n        ee_pos = outcome.get('end_effector_position', [0, 0, 0])\n\n        for axis, bounds in self.workspace_bounds.items():\n            idx = {'x': 0, 'y': 1, 'z': 2}[axis]\n            if ee_pos[idx] < bounds[0] or ee_pos[idx] > bounds[1]:\n                return False\n        return True\n\n    def _predict_collision(self, outcome):\n        \"\"\"Predict if action will cause collision.\"\"\"\n        # Simplified - would use actual collision checking\n        min_clearance = outcome.get('min_clearance', float('inf'))\n        return min_clearance < 0.05  # 5cm safety margin\n\n    def _check_stability(self, outcome):\n        \"\"\"Check if action maintains robot stability.\"\"\"\n        com_position = outcome.get('center_of_mass', [0, 0, 0])\n        support_polygon = outcome.get('support_polygon', None)\n\n        if support_polygon is None:\n            return True  # Can't verify\n\n        # Check if CoM is within support polygon\n        # Simplified 2D check\n        return self._point_in_polygon(com_position[:2], support_polygon)\n\n    def _point_in_polygon(self, point, polygon):\n        \"\"\"Check if point is inside polygon.\"\"\"\n        # Ray casting algorithm\n        x, y = point\n        n = len(polygon)\n        inside = False\n\n        p1x, p1y = polygon[0]\n        for i in range(n + 1):\n            p2x, p2y = polygon[i % n]\n            if y > min(p1y, p2y):\n                if y <= max(p1y, p2y):\n                    if x <= max(p1x, p2x):\n                        if p1y != p2y:\n                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n                        if p1x == p2x or x <= xinters:\n                            inside = not inside\n            p1x, p1y = p2x, p2y\n\n        return inside\n\n    def load_joint_limits(self):\n        \"\"\"Load joint limits from robot URDF.\"\"\"\n        # Simplified - would load from actual URDF\n        return [\n            {'min': -3.14, 'max': 3.14} for _ in range(12)\n        ]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-telemetry",children:"Monitoring and Telemetry"}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.p,{children:"Track system performance in production:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import time\nfrom collections import deque\nimport json\n\nclass VLAMonitor:\n    \"\"\"\n    Monitor VLA system performance and health.\n    \"\"\"\n\n    def __init__(self, window_size=100):\n        self.metrics = {\n            'inference_latency': deque(maxlen=window_size),\n            'action_execution_time': deque(maxlen=window_size),\n            'safety_violations': deque(maxlen=window_size),\n            'success_rate': deque(maxlen=window_size),\n            'command_processing_time': deque(maxlen=window_size)\n        }\n\n        self.start_time = time.time()\n        self.total_commands = 0\n        self.successful_commands = 0\n\n    def log_inference(self, latency_ms):\n        \"\"\"Log inference latency.\"\"\"\n        self.metrics['inference_latency'].append(latency_ms)\n\n    def log_execution(self, execution_time_ms, success):\n        \"\"\"Log action execution.\"\"\"\n        self.metrics['action_execution_time'].append(execution_time_ms)\n        self.metrics['success_rate'].append(1.0 if success else 0.0)\n\n        self.total_commands += 1\n        if success:\n            self.successful_commands += 1\n\n    def log_safety_violation(self, violation_type):\n        \"\"\"Log safety violation.\"\"\"\n        self.metrics['safety_violations'].append(violation_type)\n\n    def get_statistics(self):\n        \"\"\"Get current performance statistics.\"\"\"\n        import numpy as np\n\n        stats = {}\n\n        for metric_name, metric_data in self.metrics.items():\n            if len(metric_data) > 0:\n                if metric_name == 'safety_violations':\n                    stats[metric_name] = {\n                        'count': len(metric_data),\n                        'types': list(set(metric_data))\n                    }\n                else:\n                    stats[metric_name] = {\n                        'mean': np.mean(metric_data),\n                        'std': np.std(metric_data),\n                        'min': np.min(metric_data),\n                        'max': np.max(metric_data),\n                        'p50': np.percentile(metric_data, 50),\n                        'p95': np.percentile(metric_data, 95),\n                        'p99': np.percentile(metric_data, 99)\n                    }\n\n        # Overall metrics\n        stats['overall'] = {\n            'uptime_seconds': time.time() - self.start_time,\n            'total_commands': self.total_commands,\n            'success_rate': self.successful_commands / max(self.total_commands, 1)\n        }\n\n        return stats\n\n    def export_metrics(self, filepath):\n        \"\"\"Export metrics to file.\"\"\"\n        stats = self.get_statistics()\n        with open(filepath, 'w') as f:\n            json.dump(stats, f, indent=2)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-deployment",children:"Best Practices for Deployment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Optimization"}),": Use TensorRT and quantization for edge devices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety First"}),": Implement multi-layer safety checks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Guarantees"}),": Ensure inference meets latency requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Continuously track performance and health"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Graceful Degradation"}),": Handle failures without catastrophic consequences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Versioning"}),": Track model and code versions for reproducibility"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing"}),": Extensive simulation and real-world testing before deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human Oversight"}),": Maintain ability for human intervention"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging"}),": Comprehensive logging for debugging and improvement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Updates"}),": Support over-the-air model updates"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Successful VLA deployment requires careful attention to optimization, safety, and monitoring to ensure reliable operation on physical humanoid robots in real-world environments."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);